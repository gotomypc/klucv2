<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0052)http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/ -->
<HTML xmlns:xi="http://www.w3.org/2001/XInclude"><HEAD><META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <TITLE>Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders</TITLE>

    <META name="app" content="ppmc">
<META name="PAFTemplate" content="PPMCArticlePageT">
<META name="pdid" content="article">
<META name="db" content="pmc">
    <META name="robots" content="INDEX,NOFOLLOW,NOARCHIVE"><LINK rel="schema.DC" href="http://purl.org/DC/elements/1.0/"><META name="citation_journal_title" content="Journal of neuroscience methods"><META name="citation_title" content="Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders"><META name="citation_authors" content="Peng Wang, Frederick Barrett, Elizabeth Martin, Marina Milanova, Raquel E. Gur, Ruben C. Gur, Christian Kohler, and  Ragini Verma"><META name="citation_date" content="2008 February 15"><META name="citation_issue" content="1"><META name="citation_volume" content="168"><META name="citation_firstpage" content="224"><META name="citation_doi" content="10.1016/j.jneumeth.2007.09.030"><META name="citation_pmid" content="18045693"><META name="DC.Title" content="Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders"><META name="DC.Type" content="Text"><META name="DC.Publisher" content="NIH Public Access"><META name="DC.Contributor" content="Peng Wang"><META name="DC.Contributor" content="Frederick Barrett"><META name="DC.Contributor" content="Elizabeth Martin"><META name="DC.Contributor" content="Marina Milanova"><META name="DC.Contributor" content="Raquel E. Gur"><META name="DC.Contributor" content="Ruben C. Gur"><META name="DC.Contributor" content="Christian Kohler"><META name="DC.Contributor" content="Ragini Verma"><META name="DC.Date" content="2008 February 15"><META name="DC.Identifier" content="10.1016/j.jneumeth.2007.09.030"><META name="DC.Language" content="en"><LINK rel="stylesheet" href="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/pmcstatic.css" type="text/css"><LINK rel="stylesheet" href="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/pmcbase1.css" type="text/css"><LINK rel="stylesheet" href="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/pmcbars-slateblue.css" type="text/css"><LINK rel="stylesheet" href="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/pmcbody5.css" type="text/css"><LINK rel="stylesheet" href="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/pmcrefs1.css" type="text/css"><SCRIPT type="text/javascript" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/jquery-1.3.2.min.js"></SCRIPT><SCRIPT type="text/javascript" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/jquery.hoverIntent.min.js"></SCRIPT><SCRIPT type="text/javascript" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/common.js"></SCRIPT><SCRIPT type="text/javascript">window.name="mainwindow";</SCRIPT><LINK href="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/tileshop_pmc.1.css" type="text/css" rel="stylesheet"><SCRIPT type="text/javascript" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/noext.menu.js"></SCRIPT><LINK type="text/css" rel="stylesheet" href="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/noext-menu.css"><STYLE type="text/css">
			/* this css code is here temporarily , it will be moved during cleanup and refactor period */
			.front-matter-section,
			.section-content,
			.section-title,
			.toc-section-title,
			.back-matter-section,
			.footer-section,
			.navlink-box
			{
				width:              auto;
			}
			.pmc-watermark {
				background:         transparent url(http://www.ncbi.nlm.nih.gov//corehtml/pmc/pmcgifs/wm-nihms.gif)  repeat-y top left;
			}
			.watermark-block
			{
			    height:              300px;
			}
			.sidebar-cell ul
			{
				list-style-type:      none;
				padding:              0;
				margin:               0;
			}
			.sidefm-pmccurrent-item
			{
				padding-left:          10px;
			}

			.head1 {border: none;} /* for right alignment of the submenu with section title */

			/* article navigation toc on titles bars ends */
			a:visited {
				color:#7119B4;
			}

			html body.pmc-body {
				background-color:#F8F8F8;
				color:#212121;
				font-family:Arial,sans-serif;
				font-size:0.95em;
				font-weight:normal;
				margin:0;
				min-width:800px;
			}

			td.pmc-watermark {
				width:20px;
			}

			.section-content {
				line-height:150%;
			}

			.section-content .p {
				xoverflow: auto;
			}

			div.fm-citation-pmcid, div.fm-citation-manuscriptid, span.fm-citation-ids-label {
				color:#333333;
			}

		</STYLE><SCRIPT type="text/javascript">initRedirectClicks('/pmc/extredirect/')</SCRIPT>

    <LINK type="text/css" rel="stylesheet" href="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/panel.css">
    <LINK type="text/css" rel="stylesheet" href="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/xtheme-oldentrez.css">
    <LINK xmlns="http://www.w3.org/1999/xhtml" type="text/css" rel="stylesheet" href="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/35466.css" xml:base="http://127.0.0.1/sites/static/header_footer?Ncbi_App=pmc&amp;Db=pmc&amp;Page=article&amp;Time=2010-01-07T10:27:09-05:00&amp;Host=ptpmc2&amp;Snapshot=PPMC@131073">
  <LINK rel="shortcut icon" href="http://www.ncbi.nlm.nih.gov/favicon.ico"><META name="ncbi_phid" content="84DA06D4B45FC1A10000000000B6B774"><SCRIPT type="text/javascript"><!--
var ScriptUrl = 'http://www.ncbi.nlm.nih.gov/sites/ppmc';
var ScriptPath = '/portal/';
var objHierarchy = {"name":"PPMCLayout","type":"Layout","realname":"PPMCLayout",
"children":[{"name":"PPMCLayout.PPMCAppController","type":"Cluster","realname":"PPMCLayout.PPMCAppController",
"children":[{"name":"PPMCLayout.PPMCAppController.PPMCPageMap","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PPMCPageMap","shortname":"PPMCPageMap"},
{"name":"PPMCLayout.PPMCAppController.PPMCControllerDelegate","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PPMCControllerDelegate","shortname":"PPMCControllerDelegate"},
{"name":"PPMCLayout.PPMCAppController.PPMCAppResources","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PPMCAppResources","shortname":"PPMCAppResources"},
{"name":"PPMCLayout.PPMCAppController.PPMCNewFooterUrl","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PPMCNewFooterUrl","shortname":"PPMCNewFooterUrl"},
{"name":"PPMCLayout.PPMCAppController.DbRequest","type":"Portlet","realname":"PPMCLayout.PPMCAppController.DbRequest","shortname":"DbRequest"},
{"name":"PPMCLayout.PPMCAppController.PPMCResponseParser","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PPMCResponseParser","shortname":"PPMCResponseParser"},
{"name":"PPMCLayout.PPMCAppController.PPMCArticlePage","type":"Cluster","realname":"PPMCLayout.PPMCAppController.PPMCArticlePage",
"children":[{"name":"PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCArticlePageP","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCArticlePageP","shortname":"PPMCArticlePageP"},
{"name":"PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCCitedRefBlocks","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCCitedRefBlocks","shortname":"PPMCCitedRefBlocks"},
{"name":"PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCDiscoveryByTheseAuthors","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCDiscoveryByTheseAuthors","shortname":"PPMCDiscoveryByTheseAuthors"},
{"name":"PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCDiscoveryDbLinks","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCDiscoveryDbLinks","shortname":"PPMCDiscoveryDbLinks"},
{"name":"PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCPubmedRA","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCPubmedRA","shortname":"PPMCPubmedRA"},
{"name":"PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity","shortname":"PPMCRecentActivity"}]},
{"name":"PPMCLayout.PPMCAppController.PAFDebugPage","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PAFAppController.PAFDebugPage","shortname":"PAFDebugPage"},
{"name":"PPMCLayout.PPMCAppController.PAFTemplateResources","type":"Portlet","realname":"PPMCLayout.PPMCAppController.PAFAppController.PAFTemplateResources","shortname":"PAFTemplateResources"}]}]};
--></SCRIPT>
<SCRIPT type="text/javascript" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/portal.js"></SCRIPT><LINK type="text/css" rel="stylesheet" href="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/27687.css"><SCRIPT type="text/javascript" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/31945.js" snapshot="ppmc"></SCRIPT><SCRIPT type="text/javascript">

var ObjectLinks=[{i:0, ename: "p$ExL", esid:"*", sname: "p$ExL", ssid:"*", dname:"p$el", dsid:"0",m:"CopyValue",p:[],f: function(src, dst) {fn_CopyValue(src, dst);}}]


var ActiveNames = {"p$ExL":1, "PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.ClearHistory":0, "PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.Cmd":0, "PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.HistoryOn":0, "PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.HistoryToggle":0};
</SCRIPT></HEAD><BODY class="pmc-body">
    <A id="top" name="top"></A>
    <TABLE border="0" cellspacing="0" cellpadding="0" style="border-collapse: collapse;" id="Table2"><TBODY><TR><TD width="145" valign="top"><IMG alt="pmc logo image" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/pmc3_logo_v5.gif" usemap="#pmclogo" style="border-style: none;"><MAP name="pmclogo" id="pmclogo"><AREA alt="Journal List" href="http://www.ncbi.nlm.nih.gov/pmc/journals/" coords="65,52,137,67" shape="rect"><AREA alt="Search" href="http://www.ncbi.nlm.nih.gov/sites/entrez?db=pmc" ref="reftype=other&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|Logo&amp;TO=Entrez|Search|All%20PMC" coords="7,52,52,67" shape="rect"><AREA alt="pmc logo image" href="http://www.ncbi.nlm.nih.gov/pmc/" coords="0,0,145,75" shape="rect"></MAP></TD><TD height="75" valign="top" style="padding-left: 4px;"><DIV><IMG align="top" style="border-style: none" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/logo-nihpa.gif" alt="Logo of nihpa" usemap="#logo-imagemap"><MAP id="logo-imagemap" name="logo-imagemap"><AREA shape="rect" alt="NIHPA banner" coords="0,0,499,56" href="http://publicaccess.nih.gov/" target="pmc_ext" onclick="focuswin(&#39;pmc_ext&#39;)"><AREA shape="rect" alt="about author manuscripts" coords="0,57,249,74" href="http://www.pubmedcentral.nih.gov/about/authorms.html"><AREA shape="rect" alt="submit a manuscript" coords="250,57,499,74" href="http://www.nihms.nih.gov/" target="pmc_ext" onclick="focuswin(&#39;pmc_ext&#39;)"></MAP></DIV> </TD></TR></TBODY></TABLE>

    
    <FORM action="http://www.ncbi.nlm.nih.gov/sites/ppmc" enctype="application/x-www-form-urlencoded" onsubmit="return false;" method="post">
    <INPUT type="hidden" name="p$a" id="p$a"><INPUT type="hidden" name="p$l" id="p$l" value="PPMCLayout"><INPUT type="hidden" name="p$el" id="p$el" value="" sid="0"><INPUT type="hidden" name="p$st" id="p$st" value="ppmc"><INPUT name="SessionId" id="SessionId" value="84DA06D4B45FD4C1_0182SID" disabled="disabled" type="hidden"><INPUT name="Snapshot" id="Snapshot" value="PPMC@3.1" disabled="disabled" type="hidden"></FORM>
    
    <TABLE cellpadding="0" cellspacing="3" border="0" id="Table3">
      <TBODY><TR>
        <TD colspan="2">
          <DIV class="navlink-box navlink-box-gray"><A xmlns:str="http://exslt.org/strings" xmlns:c="http://exslt.org/common" href="http://www.ncbi.nlm.nih.gov/pmc/journals/" class="navlink">Journal List</A><SPAN> &gt; </SPAN><A xmlns:str="http://exslt.org/strings" xmlns:c="http://exslt.org/common" href="http://www.ncbi.nlm.nih.gov/sites/entrez?db=pmc&cmd=search&term=nih%20author%20manuscript[filter]" class="navlink">NIHPA Author Manuscripts</A></DIV>

        </TD>
        <TD class="format-menu">
          <H2>Formats:</H2>
          <UL xmlns:c="http://exslt.org/common"><LI><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/?report=abstract">Abstract</A></LI> | <LI class="selected">Full Text</LI> | <LI><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/pdf/nihms39199.pdf">PDF (3.4M)</A></LI></UL>

        </TD>
      </TR>
      <TR>
        <TD height="300" class="pmc-watermark">
          <!-- <div/> -->
        </TD>
        <TD valign="top" class="content-cell">
          <DIV class="front-matter-section"><TABLE cellspacing="0" cellpadding="0" width="100%"><TBODY><TR style="vertical-align: top"><TD><DIV class="fm-citation">J Neurosci Methods. Author manuscript; available in PMC 2009 February 15.</DIV><DIV class="fm-citation">Published in final edited form as:</DIV><DIV style="margin-left: 5px;"><DIV class="fm-vol-iss-date"><A class="ext-reflink" href="http://www.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&retmode=ref&cmd=prlinks&id=18045693" target="pmc_ext" onclick="focuswin(&#39;pmc_ext&#39;)" ref="reftype=publisher&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CFront%20Matter&amp;TO=Content%20Provider%7CArticle%7CRestricted%20Access&amp;rendering-type=normal"><SPAN class="citation-version"></SPAN><SPAN class="citation-abbreviation">J Neurosci Methods. </SPAN><SPAN class="citation-publication-date">2008 February 15; </SPAN><SPAN class="citation-volume">168</SPAN><SPAN class="citation-issue">(1)</SPAN><SPAN class="citation-flpages">: 224–238. </SPAN></A></DIV><SPAN class="fm-vol-iss-date">Published online 2007 October 5. </SPAN><SPAN class="fm-vol-iss-date"> </SPAN><SPAN class="fm-vol-iss-date">doi: 10.1016/j.jneumeth.2007.09.030.</SPAN></DIV></TD><TD class="fm-citation-ids"><DIV class="fm-citation-pmcid"><SPAN class="fm-citation-ids-label">PMCID: </SPAN><SPAN>PMC2238802</SPAN></DIV><DIV class="fm-citation-manuscriptid"><SPAN class="fm-citation-ids-label">NIHMSID: </SPAN><SPAN>NIHMS39199</SPAN></DIV></TD></TR></TBODY></TABLE><DIV class="fm-copyright"><A class="int-reflink" href="http://www.ncbi.nlm.nih.gov/pmc/about/copyright.html">Copyright notice</A>  and <A class="int-reflink" href="http://www.ncbi.nlm.nih.gov/About/disclaimer.html">Disclaimer</A></DIV><DIV class="fm-title">Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders</DIV><DIV class="contrib-group fm-author">Peng Wang, Frederick Barrett, Elizabeth Martin, Marina Milanova, Raquel E. Gur, Ruben C. Gur, Christian Kohler, and  Ragini Verma</DIV><DIV class="links-box"><DIV class="fm-footnote"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/rt-arrow.gif" alt="Small right arrow pointing to:" style="vertical-align: middle;"> The publisher's final edited version of this article is available at <A class="ext-reflink" href="http://www.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&retmode=ref&cmd=prlinks&id=18045693" target="pmc_ext" onclick="focuswin(&#39;pmc_ext&#39;)" ref="reftype=publisher&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CFront%20Matter&amp;TO=Content%20Provider%7CArticle%7CRestricted%20Access&amp;rendering-type=normal">J Neurosci Methods</A>.</DIV></DIV><DIV class="fm-footnote"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#FN2" class="int-reflink">Publisher's Disclaimer</A></DIV></DIV><DIV class="sec" id="__abstractid436388"><DIV class="head1 section-title" id="__abstractid436388titletitle" style="text-transform: none;"><DIV class="other-sections"><UL class="noext-menu"><LI><A class="first-link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#">&nbsp;Other Sections▼</A><UL class="submenu head1"><LI class="submenu-item current-item"><A class="" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#" onclick="return(false)" style="text-transform: none;">Abstract</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S1" class="" style="text-transform: none;">1. Introduction</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S2" class="" style="text-transform: none;">2. Related Work</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S7" class="" style="text-transform: none;">3. Methods</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S17" class="" style="text-transform: none;">4. Results</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S24" class="" style="text-transform: none;">5. Discussion and Future Work</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#__ref-listid445193" class="" style="text-transform: none;">References</A></LI></UL></LI></UL></DIV><DIV>Abstract</DIV></DIV><DIV class="section-content" id="__abstractid436388content"><!--article-meta--><DIV class="p p-first-last" id="P2"><SPAN></SPAN>Deficits in emotional expression are prominent in several neuropsychiatric disorders, including schizophrenia. Available clinical facial expression evaluations provide subjective and qualitative measurements, which are based on static 2D images that do not capture the temporal dynamics and subtleties of expression changes. Therefore, there is a need for automated, objective and quantitative measurements of facial expressions captured using videos. This paper presents a computational framework that creates probabilistic expression profiles for video data and can potentially help to automatically quantify emotional expression differences between patients with neuropsychiatric disorders and healthy controls. Our method automatically detects and tracks facial landmarks in videos, and then extracts geometric features to characterize facial expression changes. To analyze temporal facial expression changes, we employ probabilistic classifiers that analyze facial expressions in individual frames, and then propagate the probabilities throughout the video to capture the temporal characteristics of facial expressions. The applications of our method to healthy controls and case studies of patients with schizophrenia and Asperger’s syndrome demonstrate the capability of the video-based expression analysis method in capturing subtleties of facial expression. Such results can pave the way for a video based method for quantitative analysis of facial expressions in clinical research of disorders that cause affective deficits.</DIV><DIV class="p"><SPAN class="kwd-label">Keywords: </SPAN><SPAN class="kwd-text">facial expression, video analysis, schizophrenia, affective deficits, pattern classification</SPAN></DIV></DIV></DIV><DIV class="sec" id="S1"><SPAN></SPAN><DIV class="head1 section-title" id="S1titletitle" style="text-transform: none;"><DIV class="other-sections"><UL class="noext-menu"><LI><A class="first-link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#">&nbsp;Other Sections▼</A><UL class="submenu head1"><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#__abstractid436388" class="" style="text-transform: none;">Abstract</A></LI><LI class="submenu-item current-item"><A class="" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#" onclick="return(false)" style="text-transform: none;">1. Introduction</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S2" class="" style="text-transform: none;">2. Related Work</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S7" class="" style="text-transform: none;">3. Methods</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S17" class="" style="text-transform: none;">4. Results</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S24" class="" style="text-transform: none;">5. Discussion and Future Work</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#__ref-listid445193" class="" style="text-transform: none;">References</A></LI></UL></LI></UL></DIV><DIV>1. Introduction</DIV></DIV><DIV class="section-content" id="S1content"><DIV class="p p-first" id="P3"><SPAN></SPAN>Facial expressions have been used in clinical research to study deficits in emotional expression and social cognition in neuropsychiatric disorders [<A href="http://www.ncbi.nlm.nih.gov/pubmed/3291095" rid="R1" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">1</A>–<A href="http://www.ncbi.nlm.nih.gov/pubmed/9718632" rid="R4" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">4</A>]. Specifically, patients with schizophrenia often demonstrate two types of impairments in facial expressions: “flat affect” and “inappropriate affect” [<A href="http://www.ncbi.nlm.nih.gov/pubmed/16452608" rid="R5" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">5</A>]. However, most of the current clinical methods, such as the scale for assessment of negative symptoms (SANS [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R6" rid="R6" class="cite-reflink bibr popnode">6</A>]), are based on subjective ratings and therefore provide qualitative measurements. They also require extensive human expertise and interpretation. This underlines the need for automated, objective and quantitative measurements of facial expression. We previously reported a method for quantifying facial expressions based on static images [<A href="http://www.ncbi.nlm.nih.gov/pubmed/15585289" rid="R14" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">14</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R15" rid="R15" class="cite-reflink bibr popnode">15</A>]. However, temporal information plays an important role in understanding facial expressions because emotion processing is naturally a temporal procedure. Therefore, facial expression analysis from static 2D images lacks the temporal component, which is essential to capture subtle changes in expression. Although video-based acquisition has been employed in the examination of facial emotion expression [<A href="http://www.ncbi.nlm.nih.gov/pubmed/17563202" rid="R7" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">7</A>], currently there is no objective and automated way of facial expression analysis for the study of neuropsychiatric disorders, particularly due to the large volume of data that makes human analysis prohibitive. In this paper, we present a computational framework that uses videos to automatically analyze facial expressions and can be used to characterize impairments in such neuropsychiatric disorders.</DIV><DIV class="p" id="P4"><SPAN></SPAN>The merits of automated facial expression analysis (AFEA) are two-fold: using it can avoid intensive human efforts, and can provide unified quantitative results. There are already many AFEA methods being presented in both clinical and computer vision communities [<A href="http://www.ncbi.nlm.nih.gov/pubmed/1390955" rid="R8" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">8</A>–<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R13" rid="R13" class="cite-reflink bibr popnode">13</A>]. Most of the current AFEA methods focus on the recognition of posed facial expressions with application to human computer interaction tasks, and only a few of them have been applied to clinical studies [<A href="http://www.ncbi.nlm.nih.gov/pubmed/15585289" rid="R14" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">14</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R15" rid="R15" class="cite-reflink bibr popnode">15</A>]. In previous work on expression quantification [<A href="http://www.ncbi.nlm.nih.gov/pubmed/15585289" rid="R14" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">14</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R15" rid="R15" class="cite-reflink bibr popnode">15</A>], the expression changes were modeled using elastic shape transformations between the face of a neutral template and the corresponding emotionally expressive face. Again, as most of the current AFEA methods, this approach is based on static 2D images without any temporal component.</DIV><DIV class="p" id="P5"><SPAN></SPAN>In this paper, we present a computational framework that uses videos for the analysis of facial expression changes. This framework explores the dynamic information that is not captured by static images during emotion processing, and provides computationally robust results with potential clinical applicability. Broadly, our computational framework includes the detection of faces in videos, which are then tracked through the video, incorporating shape changes. Based on tracking results, features are extracted from faces to create probabilistic facial expression classifiers. The probabilistic outputs of facial expression classifiers are propagated throughout the video, to create probabilistic profiles of facial expressions. Probabilistic profiles contain dynamic information of facial expressions, based on which quantitative measures are extracted for analysis. As an application of this framework, such quantitative measurements for facial expressions could be correlated with clinical ratings to study the facial expression deficits in neuropsychiatric disorders. To our knowledge, the presented framework is the first to apply video based automated facial expression analysis in neuropsychiatric research.</DIV><DIV class="p p-last" id="P6"><SPAN></SPAN>The rest of the paper is organized as follows: In Section 2, previous related work is reviewed. Our computational framework is presented in Section 3. The experimental results are provided in Section 4. We discuss the results and conclude in Section 5.</DIV></DIV></DIV><DIV class="sec" id="S2"><SPAN></SPAN><DIV class="head1 section-title" id="S2titletitle" style="text-transform: none;"><DIV class="other-sections"><UL class="noext-menu"><LI><A class="first-link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#">&nbsp;Other Sections▼</A><UL class="submenu head1"><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#__abstractid436388" class="" style="text-transform: none;">Abstract</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S1" class="" style="text-transform: none;">1. Introduction</A></LI><LI class="submenu-item current-item"><A class="" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#" onclick="return(false)" style="text-transform: none;">2. Related Work</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S7" class="" style="text-transform: none;">3. Methods</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S17" class="" style="text-transform: none;">4. Results</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S24" class="" style="text-transform: none;">5. Discussion and Future Work</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#__ref-listid445193" class="" style="text-transform: none;">References</A></LI></UL></LI></UL></DIV><DIV>2. Related Work</DIV></DIV><DIV class="section-content" id="S2content"><DIV id="S3" class="sec sec-first"><SPAN></SPAN><DIV class="head2 head-separate">2.1 Clinical Facial Expression Analysis</DIV><DIV class="p p-first-last" id="P7"><SPAN></SPAN>In clinical research, facial expressions are usually studied using 2D images that are described in two ways: either as a combination of muscular movements or as universal global expressions. The Facial Action Coding System (FACS) has been developed to describe facial expressions using a combination of action units (AU) [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R16" rid="R16" class="cite-reflink bibr popnode">16</A>]. Each action unit corresponds to a specific muscular activity that produces momentary changes in facial appearance. The global facial expression handles the expressions as a whole without breaking up into AUs. The most commonly studied universal expressions include happiness, sadness, anger and fear, which are referred to as universal emotions. While most of the work has been on static 2D images, the Facial Expression Coding System (FACES) [<A href="http://www.ncbi.nlm.nih.gov/pubmed/17563202" rid="R7" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">7</A>] has been designed to analyze videos of facial expressions, in terms of the duration, content and valence of universal expressions. However, these methods need intensive human intervention to rate the images and videos of facial expressions. Such rating methods are prone to subjective errors, and have difficulties in providing unified quantitative measurements. There is need for automated, objective and quantitative measurements of facial expressions.</DIV></DIV><DIV id="S4" class="sec sec-last"><SPAN></SPAN><DIV class="head2 head-separate">2.2 Automated Facial Expression Analysis</DIV><DIV class="p p-first-last" id="P8"><SPAN></SPAN>Automated facial expression analysis (AFEA) allows computers to automatically provide quantitative measurements of facial expressions. Several factors have contributed towards making AFEA challenging. First, facial expressions vary across individuals due to the differences of the facial appearance, degree of facial plasticity, morphology and frequency of facial expressions [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R13" rid="R13" class="cite-reflink bibr popnode">13</A>]. Second, it is difficult to quantify the intensity of facial expressions, especially when they are subtle. In FACS, a set of rules are used to score AU intensities [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R16" rid="R16" class="cite-reflink bibr popnode">16</A>]. However, such criteria are subjective to the rater; therefore it is difficult to extend the measurements to computer-based facial expression analysis, although there have been methods to automatically detect AUs [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R11" rid="R11" class="cite-reflink bibr popnode">11</A>]. Many AFEA methods have been developed recently to address such problems [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R11" rid="R11" class="cite-reflink bibr popnode">11</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R12" rid="R12" class="cite-reflink bibr popnode">12</A>]. These methods can be categorized as image-based, video-based and 3D surface based, according to the data used. Below we summarize some typical image-based and video-based facial expression analysis methods.</DIV><DIV id="S5" class="sec"><SPAN></SPAN><SPAN class="head3">2.2.1 Image Based Methods </SPAN> <DIV class="p p-first-last" id="P9"><SPAN></SPAN>Image-based methods extract features from individual images, and create classifiers to recognize facial expressions. Commonly used are geometric features, texture features, and their combinations. Geometric features represent the spatial information of facial expressions, such as positions of eyes and mouth, the distance between two eyebrows. The geometric features used by Tian et al. in [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R17" rid="R17" class="cite-reflink bibr popnode">17</A>] are grouped into permanent and transient. The permanent features include positions of lips, eyes, brows, cheeks and furrows that have become permanent with age. The transient features include facial lines and furrows that are not present at rest but appear with facial expressions [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R17" rid="R17" class="cite-reflink bibr popnode">17</A>]. The texture features include image intensity[<A href="http://www.ncbi.nlm.nih.gov/pubmed/10194972" rid="R18" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">18</A>], image difference[<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R19" rid="R19" class="cite-reflink bibr popnode">19</A>], edge [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R17" rid="R17" class="cite-reflink bibr popnode">17</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R20" rid="R20" class="cite-reflink bibr popnode">20</A>], and wavelets [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R21" rid="R21" class="cite-reflink bibr popnode">21</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R22" rid="R22" class="cite-reflink bibr popnode">22</A>]. To recognize subtle facial expressions, both features computed by using principal components and image difference usually require precise alignment, not readily feasible in real world applications. The edge features are often used to describe furrows and lines caused by facial expressions, but are difficult to detect for subtle expressions. Gabor wavelets calculated from facial appearance describe both spatial and frequency information for image analysis, and have shown capability in face recognition and facial feature tracking [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R23" rid="R23" class="cite-reflink bibr popnode">23</A>], as well as facial expression recognition [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R21" rid="R21" class="cite-reflink bibr popnode">21</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R22" rid="R22" class="cite-reflink bibr popnode">22</A>]. Furthermore, experiments [<A href="http://www.ncbi.nlm.nih.gov/pubmed/10194972" rid="R18" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">18</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R24" rid="R24" class="cite-reflink bibr popnode">24</A>] demonstrate that the fusion of appearance features (Gabor wavelets or PCA features) and geometric features can provide better accuracy than using either of them alone. To recognize facial expressions, extracted features are input to facial expression classifiers, such as the Nearest Neighbor classifier [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R19" rid="R19" class="cite-reflink bibr popnode">19</A>], Neural Networks [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R17" rid="R17" class="cite-reflink bibr popnode">17</A>], SVM [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R22" rid="R22" class="cite-reflink bibr popnode">22</A>], Bayesian Networks [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R25" rid="R25" class="cite-reflink bibr popnode">25</A>], and AdaBoost classifier [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R22" rid="R22" class="cite-reflink bibr popnode">22</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R26" rid="R26" class="cite-reflink bibr popnode">26</A>].</DIV></DIV><DIV id="S6" class="sec sec-last"><SPAN></SPAN><SPAN class="head3">2.2.2 Video-Based Methods </SPAN> <DIV class="p p-first" id="P10"><SPAN></SPAN>It is claimed that temporal information can improve the accuracy of facial expression recognition over using static images [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R27" rid="R27" class="cite-reflink bibr popnode">27</A>]. However, only few video-based methods have been developed to use the temporal information of facial expressions [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R22" rid="R22" class="cite-reflink bibr popnode">22</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R27" rid="R27" class="cite-reflink bibr popnode">27</A>–<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R31" rid="R31" class="cite-reflink bibr popnode">31</A>]. In the work of Yacoob et al. [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R28" rid="R28" class="cite-reflink bibr popnode">28</A>], each facial expression is divided into three segments: the beginning, the apex and the ending. Rules are defined to determine the temporal model of facial expressions. Such rules are ad-hoc, and cannot be generalized to complex environments. In the work of Cohen et al. [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R27" rid="R27" class="cite-reflink bibr popnode">27</A>], facial expressions are represented in terms of magnitudes of predefined facial motions, so called Motion-Units (MU). A Tree-Augmented-Naive Bayes classifier is first used to recognize facial expressions at the level of static images, and then a multi-level Hidden Markov Model (HMM) structure is applied to recognize facial expressions at the level of video sequences. Yeasin et al. also present a two-stage approach to recognize facial expression and its intensity in video using optical flow [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R29" rid="R29" class="cite-reflink bibr popnode">29</A>]. Another example of using HMM for facial expression analysis can be found in [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R30" rid="R30" class="cite-reflink bibr popnode">30</A>]. Besides HMM, the sampling-based probabilistic tracking methods, known as “particle filtering” or “Condensation”, are also used to track facial expression in video sequence [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R31" rid="R31" class="cite-reflink bibr popnode">31</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R32" rid="R32" class="cite-reflink bibr popnode">32</A>]. Manifold subspace features have been applied for video based facial expression analysis. However, in their methods, a separate manifold is built for each subject, and the subjects appear in both training and testing sequences. It is unclear that such specifically learned manifolds can be generalized to different subjects, since it is observed that their manifolds show different structures [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R31" rid="R31" class="cite-reflink bibr popnode">31</A>].</DIV><DIV class="p" id="P11"><SPAN></SPAN>An important facet in video-based methods is how to maintain accurate tracking throughout the video sequence. A wide range of deformable models, such as muscle-based models [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R33" rid="R33" class="cite-reflink bibr popnode">33</A>], a 3D wireframe model [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R27" rid="R27" class="cite-reflink bibr popnode">27</A>], a facial mesh model [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R34" rid="R34" class="cite-reflink bibr popnode">34</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R35" rid="R35" class="cite-reflink bibr popnode">35</A>], a potential net model [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R36" rid="R36" class="cite-reflink bibr popnode">36</A>], ASM [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R37" rid="R37" class="cite-reflink bibr popnode">37</A>], and a geometry-based shape model [<A href="http://www.ncbi.nlm.nih.gov/pubmed/15585289" rid="R14" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">14</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R38" rid="R38" class="cite-reflink bibr popnode">38</A>], are used to track facial features in video. Although it has been demonstrated that a sophisticated deformable model can improve facial tracking accuracy, thereby improving facial expression analysis accuracy [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R39" rid="R39" class="cite-reflink bibr popnode">39</A>], there are no comprehensive experiments showing which deformable model is superior to the others.</DIV><DIV class="p p-last" id="P12"><SPAN></SPAN>In summary, video based methods can capture subtle changes and temporal trends of facial expression, which cannot be achieved by static image based methods. Due to the large amount of data in videos, a fully automated method for analysis is required. In the following sections, we first present a framework that is able to quantify the facial expression changes in video, and then describe normative data on healthy people, and finally apply the method in two illustrative patients to examine its potential for research in neuropsychiatric disorders.</DIV></DIV></DIV></DIV></DIV><DIV class="sec" id="S7"><SPAN></SPAN><DIV class="head1 section-title" id="S7titletitle" style="text-transform: none;"><DIV class="other-sections"><UL class="noext-menu"><LI><A class="first-link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#">&nbsp;Other Sections▼</A><UL class="submenu head1"><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#__abstractid436388" class="" style="text-transform: none;">Abstract</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S1" class="" style="text-transform: none;">1. Introduction</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S2" class="" style="text-transform: none;">2. Related Work</A></LI><LI class="submenu-item current-item"><A class="" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#" onclick="return(false)" style="text-transform: none;">3. Methods</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S17" class="" style="text-transform: none;">4. Results</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S24" class="" style="text-transform: none;">5. Discussion and Future Work</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#__ref-listid445193" class="" style="text-transform: none;">References</A></LI></UL></LI></UL></DIV><DIV>3. Methods</DIV></DIV><DIV class="section-content" id="S7content"><DIV class="p p-first-last" id="P13"><SPAN></SPAN>This section presents our computational framework for facial expression analysis using video data. We provide an overview of the framework in Section 3.1, with further details in subsequent subsections.</DIV><DIV id="S8" class="sec"><SPAN></SPAN><DIV class="head2 head-separate">3.1 A Framework of Quantitative Facial Expression Analysis in Video</DIV><DIV class="p p-first-last" id="P14"><SPAN></SPAN>Our framework for automated facial expression analysis of video data comprises the following components: 1) detecting landmarks that define the facial shape, and tracking landmarks and hence the facial changes due to expressions; 2) feature extraction based on these landmarks; 3) creation of classifiers based on extracted features, and probabilistic classification at each frame of the video sequence; and 4) probabilistic propagation of facial expressions throughout the video. We first apply a face detector and a landmark detector to automatically locate landmarks in videos. Based on these detected landmarks, the method further extracts geometric features to characterize the face shape changes caused by facial expressions. Geometric features are normalized, which are demonstrated to be robust to skin color and illumination variations, and are input to facial expression classifiers for analysis. Therefore, the third part of the method is the creation of probabilistic classifiers using the extracted features. Offline–trained support vector machines (SVMs) (a type of non-linear pattern classification technique) are employed to obtain the likelihood probability of each facial expression. Since the probabilistic classifiers only describe the facial expressions at individual frames, our framework further propagates the measurements at individual frames throughout videos using a sequential Bayesian inference scheme, to obtain a representation of facial expression changes in the whole video in the form of a temporal <EM>probabilistic profile of facial expressions</EM>. The computational framework is general, and applicable to all types of participants, for video based facial expression analysis. The method is applied to a group of healthy people and representative patients with neuropsychiatric disorders, and measurements extracted from probabilistic profile of facial expressions are expected to distinguish between patients and controls.</DIV></DIV><DIV id="S9" class="sec"><SPAN></SPAN><DIV class="head2 head-separate">3.2 Landmark Detection and Tracking in Video</DIV><DIV class="p p-first-last" id="P15"><SPAN></SPAN>In this section, we present our landmark detection and tracking method. In the work of [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R15" rid="R15" class="cite-reflink bibr popnode">15</A>], the face region is manually outlined to obtain the deformation between faces with expression and neutral faces for analysis. However, manual labeling is time-consuming, and subjective to the person who labels the face. Especially in our study, the video of each participant may contain different facial expressions, up to 10,000 to 20,000 frames. Thus, it is a formidable task to manually mark all the face shapes in the videos. An automated system is desirable to perform the landmark points detection and tracking with minimum human intervention. To automate the process, we first detect the face and facial landmarks in the starting frame of the video using a face detector and an Active Appearance Model (AAM) [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R40" rid="R40" class="cite-reflink bibr popnode">40</A>], and then track the landmark points in all the remaining frames. In the meantime, the face detector is running through the video to monitor the tracking, and re-initializes the tracker when participants' faces are out of the frontal view or occluded when the facial expression analysis cannot be performed. The whole scheme is illustrated in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F1/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 1</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f1.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f1.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 1" title="Figure 1"></SPAN></SPAN></A>.</DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F1" name="F1"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F1/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f1.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f1.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 1" title="Figure 1"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f1.gif" class="icon-reflink small-thumb" alt="Figure 1" title="Figure 1"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F1/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" target="figure"><STRONG>Figure 1</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Landmark detection and tracking in videos</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV><DIV id="S10" class="sec"><SPAN></SPAN><SPAN class="head3">3.2.1 Face Detection </SPAN> <DIV class="p p-first-last" id="P16"><SPAN></SPAN>In our method, the face is automatically detected in the first frame of the video. Many face detection methods have been recently developed [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R41" rid="R41" class="cite-reflink bibr popnode">41</A>]. Among current methods, the AdaBoost based methods achieve excellent detection accuracy as well as real-time speed [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R42" rid="R42" class="cite-reflink bibr popnode">42</A>–<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R44" rid="R44" class="cite-reflink bibr popnode">44</A>]. Here we have applied AdaBoost algorithm with Haar features, to detect frontal and near-frontal faces[<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R42" rid="R42" class="cite-reflink bibr popnode">42</A>]. In this method, critical Haar features are sequentially selected from an over-complete feature set, which may contain more than 45,000 Haar wavelet features. Threshold classifiers are learned from the selected features, and are combined by AdaBoost. With a cascade structure [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R42" rid="R42" class="cite-reflink bibr popnode">42</A>], AdaBoost-based frontal face detection methods can achieve real-time speed (i.e., above 15 frames per second) with accuracy comparable to other methods. Note that our face detector aims at detecting only frontal faces, since our facial expression analysis is only applied to frontal faces. <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F2/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 2</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f2.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f2.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 2" title="Figure 2"></SPAN></SPAN></A> shows face detection result in the first frame of a video.</DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F2" name="F2"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F2/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f2.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f2.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 2" title="Figure 2"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f2.gif" class="icon-reflink small-thumb" alt="Figure 2" title="Figure 2"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F2/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Figure 2</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Face detection at the first frame</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV></DIV><DIV id="S11" class="sec"><SPAN></SPAN><SPAN class="head3">3.2.2 Landmark Detection and Tracking </SPAN> <DIV class="p p-first" id="P17"><SPAN></SPAN>Inside each detected face, our method further identifies important landmarks to characterize facial expression changes. Active appearance model (AAM) [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R40" rid="R40" class="cite-reflink bibr popnode">40</A>] locates these landmark points. AAM is a statistical method to model face appearance as well as face shape. In AAM, the face shape is represented by a set of landmarks, and the face texture is the image intensity or color of the whole face region. AAM face model combines the principal components from face texture and shape to formalize a vector, and then apply an additional principal component analysis (PCA) to further reduce the feature dimensionality. AAM models can be learned offline from collected annotated training samples. To locate landmarks in a given image with unknown faces, an efficient method has been developed in [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R40" rid="R40" class="cite-reflink bibr popnode">40</A>] to identify landmarks in images by minimizing the error between original face and its PCA reconstruction.</DIV><DIV class="p" id="P18"><SPAN></SPAN>In our method, we define the face shape using 58 landmarks, as shown in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F3/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 3(a)</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f3.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f3.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 3" title="Figure 3"></SPAN></SPAN></A>. Among those landmarks, 5 points are defined on each eye brow, 8 points are defined on each eye, 11 points are defined on the nose, 8 points are defined on the mouth, and 13 points are defined on the face outline. The face texture in our AAM is defined as the RGB color values of the face, which are transformed on the mean shape. We collect about 100 face images with manually annotated landmarks, to obtained AAM models. Our implementation of AAM is modified from [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R45" rid="R45" class="cite-reflink bibr popnode">45</A>]. For given images with unknown faces, our method automatically detects the landmarks using the trained AAM model. <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F3/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 3(b)</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f3.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f3.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 3" title="Figure 3"></SPAN></SPAN></A> shows the detected landmarks at the first frame of video.</DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F3" name="F3"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F3/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f3.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f3.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 3" title="Figure 3"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f3.gif" class="icon-reflink small-thumb" alt="Figure 3" title="Figure 3"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F3/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Figure 3</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Definition of landmarks and their detection. (a) 58 landmarks defined on face; (b) the landmarks detected at the first frame</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="p p-last" id="P19"><SPAN></SPAN>AAM is also used to track the landmark points in the rest of the video. At each frame, the face shape is initialized with the shape at the previous frame, and then AAM is applied to update the face shape at the current frame. Compared to independent landmark detection at individual frames, the AAM tracking speeds up the searching procedure by limiting the searching only around the previous location, given the assumption that the face moves smoothly. <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F4/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 4</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f4.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f4.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 4" title="Figure 4"></SPAN></SPAN></A> shows the tracked landmarks in the video.</DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F4" name="F4"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F4/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f4.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f4.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 4" title="Figure 4"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f4.gif" class="icon-reflink small-thumb" alt="Figure 4" title="Figure 4"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F4/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Figure 4</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Landmark tracking results in the video</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV></DIV><DIV id="S12" class="sec sec-last"><SPAN></SPAN><SPAN class="head3">3.2.3 Combination of Face Detection and Landmark Detection </SPAN> <DIV class="p p-first-last" id="P20"><SPAN></SPAN>Although participants are instructed to restrict their head movement during data capture, the faces of participants could still be out of the frontal view sometimes. Such cases will fail during face tracking as well as in the facial expression analysis. To address such a problem, face detection is combined with landmark tracking such that landmarks detected can be monitored. The frontal face detector will lose detection when the faces are out of the frontal view or are occluded. Then the AAM tracking will be stopped. The face detector will keep searching frames until the face is back to its frontal view, or the occlusion is over. Then the AAM tracker is re-initialized inside the detected face region. In our experiments, only about 1.4% of frames in all participants have shown non-frontal faces. The faces out of frontal view will be excluded from the subsequent facial expression analysis. <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F5/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 5</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f5.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f5.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 5" title="Figure 5"></SPAN></SPAN></A> shows how face detection can find the face that is out of view and re-initialize the face tracking.</DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F5" name="F5"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F5/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f5.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f5.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 5" title="Figure 5"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f5.gif" class="icon-reflink small-thumb" alt="Figure 5" title="Figure 5"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F5/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Figure 5</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Landmark tracking combined with face detection. (a) tracking when face is detected; (b) AAM tracking is stopped when face is out of view; (c) tracking is re-initialized when the face is back to frontal view</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV></DIV></DIV><DIV id="S13" class="sec"><SPAN></SPAN><DIV class="head2 head-separate">3.3. Facial Expression Feature Extraction</DIV><DIV class="p p-first" id="P21"><SPAN></SPAN>Geometric features are extracted from landmarks to characterize facial expression changes. The first type of geometric features are the area changes of 28 regions defined by 58 landmark points, as illustrated in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F6/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 6(a)</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f6.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f6.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 6" title="Figure 6"></SPAN></SPAN></A>. Such areas describe the global changes caused by facial expressions. There are also some facial actions that are closely related to expression changes. Such facial actions include eye opening, mouth opening, mouth corner movement and eyebrow movement. To specifically describe such actions, we define another type of geometric features, which measure distances between some landmark points, as illustrated in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F6/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 6(b)</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f6.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f6.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 6" title="Figure 6"></SPAN></SPAN></A>.</DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F6" name="F6"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F6/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f6.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f6.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 6" title="Figure 6"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f6.gif" class="icon-reflink small-thumb" alt="Figure 6" title="Figure 6"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F6/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Figure 6</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Geometric features defined on landmarks for expression analysis. (a) 28 regions defined on landmarks; (b) distance features characterizing expression changes</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="p p-last" id="P22"><SPAN></SPAN>To eliminate effects of individual differences in facial expressions, extracted features are normalized in several ways. First, each face shape is normalized to the same scale. We use the face width to normalize faces, since it does not vary with facial expression changes. Second, geometric features are normalized by each subject's neutral faces. For example, each 2D geometric feature is divided by its corresponding value at the neutral expression of the same person. Thus, the geometric features only reflect the ratio changes of 2D face geometry, and individual topological differences are canceled. Finally, all the feature values are normalized to z-scores for subsequent analysis.</DIV></DIV><DIV id="S14" class="sec"><SPAN></SPAN><DIV class="head2 head-separate">3.4 Facial Expression Classifiers</DIV><DIV class="p p-first" id="P23"><SPAN></SPAN>To quantify facial expressions, extracted features are used to train facial expression classifiers. We adopt support vector machines (SVM) [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R46" rid="R46" class="cite-reflink bibr popnode">46</A>] as a pattern classification method to train classifiers. SVM is a binary classifier, which can separate two classes by projecting original data onto a high dimensional space through kernel functions. It provides good accuracy and generalization capability. At the training stage, SVM requires training samples to obtain class boundaries. At the classification stage, for a new data point, SVM returns a numeric result that represents the distance from the feature point to the class boundary. There are some efforts to interpret SVM outputs from the probabilistic point of view [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R47" rid="R47" class="cite-reflink bibr popnode">47</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R48" rid="R48" class="cite-reflink bibr popnode">48</A>]. A direct method is to fit the output of SVM into parametric probability models [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R48" rid="R48" class="cite-reflink bibr popnode">48</A>]. By assuming the distance output by SVM as a Gaussian likelihood model, the posterior probabilities can be directly fit with the sigmoid function as shown in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#FD1" rid="FD1" class="cite-reflink disp-formula">Eqn (1)</A>: 
<DIV class="disp-formula" id="FD1"><TABLE width="100%" border="0" cellspacing="5" cellpadding="5"><TBODY><TR><TD style="width:95%"><DIV class="eqn-image"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/M1" style="vertical-align: middle; background: #F8F8F8;padding: 2pt;border-style: none;" class="equation" alt="equation M1" title="equation M1"></DIV></TD><TD style="width:5%"><DIV class="eqn-id"><SPAN style="white-space: nowrap;">(1)</SPAN></DIV></TD></TR></TBODY></TABLE></DIV> where <EM>x</EM> is the class label, <EM>Z</EM> is the output of SVM. The parameters <EM>μ<SUB>i</SUB></EM>,<EM>σ<SUB>i</SUB>, A<SUB>i</SUB></EM>, <EM>B<SUB>i</SUB></EM> are estimated from training data. We are mainly interested in the likelihood probability <EM>p</EM>(<EM>Z</EM> | <EM>x</EM>), which will be used for the later Bayesian probability propagation.</DIV><DIV class="p p-last" id="P24"><SPAN></SPAN>The label <EM>x</EM> refers to the facial expression, and <EM>Z</EM> is the SVM output of extracted features. The class label <EM>x</EM> takes discrete values, i.e. <EM>x</EM> = <EM>i</EM> indicates the existence of the <EM>i</EM>-th facial expression. <EM>p</EM>(<EM>Z</EM> | <EM>x</EM> = <EM>i</EM>) and <EM>p</EM>(<EM>x</EM> = <EM>i</EM> | <EM>Z</EM>) are the likelihood and posterior probability of the <EM>i</EM>-th facial expression respectively. However, SVM is essentially a binary classifier, while facial expression analysis is a multi-class problem as there are more than 2 facial expressions. There are usually two strategies, i.e., “one-against-another” and “one-against-all”, to extend binary classifiers for a multi-class problem. In the “one-against-another” strategy, multiple binary classifiers are trained for each pair of classes. If there are <EM>k</EM> classes, there will be <EM>k(k</EM>−<EM>1)/2</EM> binary classifiers. The final decision is made based on majority voting over all the binary classifiers. In another “one-against-all” strategy, <EM>k</EM> binary classifiers are trained for <EM>k</EM> classes, with each binary classifier trained to separate one facial expression from the other facial expressions. It is shown in [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R49" rid="R49" class="cite-reflink bibr popnode">49</A>] that the “one-against-another” significantly increases the computational complexity, but improves the accuracy only slightly. Therefore, we apply the “one-against-all” strategy, i.e., training one SVM classifier for each expression using extracted features. For analysis of new data, the outputs from SVM classifiers, <EM>p</EM>(<EM>Z</EM> | <EM>x</EM> = <EM>i</EM>), will be used for the probabilistic propagation in video sequences.</DIV></DIV><DIV id="S15" class="sec"><SPAN></SPAN><DIV class="head2 head-separate">3.5 Probabilistic Propagation in Video: Creation of Probabilistic Profile of Facial Expressions</DIV><DIV class="p p-first" id="P25"><SPAN></SPAN>The probabilistic outputs of facial expression classifiers, <EM>p</EM>(<EM>z</EM> | <EM>x<SUB>i</SUB></EM>), model facial expressions at individual frames only, but have not fully utilized the temporal information of facial expressions in videos. We apply a sequential Bayesian estimation scheme to propagate the posterior probabilities of facial expressions throughout the whole video. The sequential Bayesian estimation and its Monte Carlo derivations have been widely used in visual tracking [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R6" rid="R6" class="cite-reflink bibr popnode">6</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R29" rid="R29" class="cite-reflink bibr popnode">29</A>], as they can handle sequential inference problems effectively and elegantly. Our method applies the sequential Bayesian estimation to infer the posterior probability <EM>P</EM>(<EM>x<SUB>t</SUB></EM> | <EM>Z</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM>) of facial expressions in video. In <EM>P</EM>(<EM>x<SUB>t</SUB></EM> | <EM>Z</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM>), <EM>x<SUB>t</SUB></EM> refers to the facial expression at the <EM>t</EM>-th frame, and Z<SUB>1:</SUB><EM><SUB>t</SUB></EM> represents the history of features extracted from frame <EM>1</EM> to frame <EM>t</EM>. To infer <EM>P</EM>(<EM>x<SUB>t</SUB></EM> | <EM>Z</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM>) from individual frames, a “dynamic model” is needed to describe the temporal relationship between facial expressions is needed. Such a dynamic model is denoted as <EM>P</EM>(<EM>x<SUB>t</SUB></EM> | <EM>x</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM><SUB>−1</SUB>). Usually, there are two assumptions made in the sequential Bayesian estimation for purpose of simplicity: <EM>P</EM>(<EM>x<SUB>t</SUB></EM>|<EM>x</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM><SUB>−1</SUB>) = <EM>P</EM>(<EM>x<SUB>t</SUB></EM>|<EM>x<SUB>t</SUB></EM><SUB>−1</SUB>) and <EM>P</EM>(<EM>Z<SUB>t</SUB>|x</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM>) = <EM>P</EM>(<EM>Z<SUB>t</SUB></EM>|<EM>x<SUB>t</SUB></EM>). Such assumptions are called Markov properties, and have been widely adopted in the sequential inference. A graphical model that illustrates our sequential Bayesian estimation is shown in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F7/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 7</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f7.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f7.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 7" title="Figure 7"></SPAN></SPAN></A>. With the assumptions of Markov property, posterior probabilities can be estimated from a measurement model <EM>P</EM>(<EM>Z<SUB>t</SUB></EM> | <EM>x<SUB>t</SUB></EM>) and a propagated prior <EM>P</EM>(<EM>x<SUB>t</SUB></EM><SUB>−1</SUB> | <EM>Z</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM><SUB>−1</SUB>), based on Bayes rule, as <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#FD2" rid="FD2" class="cite-reflink disp-formula">Eqn (2)</A>: 
<DIV class="disp-formula" id="FD2"><TABLE width="100%" border="0" cellspacing="5" cellpadding="5"><TBODY><TR><TD style="width:95%"><DIV class="eqn-image"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/M2" style="vertical-align: middle; background: #F8F8F8;padding: 2pt;border-style: none;" class="equation" alt="equation M2" title="equation M2"></DIV></TD><TD style="width:5%"><DIV class="eqn-id"><SPAN style="white-space: nowrap;">(2)</SPAN></DIV></TD></TR></TBODY></TABLE></DIV> where 
<SPAN id="__inline-formulaid572649"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/M3" style="vertical-align: middle; background: #F8F8F8;padding: 2pt;border-style: none;" class="equation" alt="equation M3" title="equation M3"></SPAN> is a normalization constant that ensures that the summation of probability equals to 1. As shown in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#FD2" rid="FD2" class="cite-reflink disp-formula">Eqn (2)</A>, the posterior probability <EM>P</EM>(<EM>x<SUB>t</SUB></EM> | <EM>Z</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM>) is sequentially estimated from the previous probability <EM>P</EM>(<EM>x<SUB>t</SUB></EM><SUB>−1</SUB> | <EM>Z</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM><SUB>−1</SUB>).</DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F7" name="F7"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F7/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f7.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f7.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 7" title="Figure 7"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f7.gif" class="icon-reflink small-thumb" alt="Figure 7" title="Figure 7"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F7/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Figure 7</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>A graphical model for facial expression inference in video.</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="p p-last" id="P26"><SPAN></SPAN>For the facial expression analysis of any participant using video, the likelihood measurement <EM>P</EM>(<EM>Z<SUB>t</SUB></EM> | <EM>x<SUB>t</SUB></EM>) is obtained by inputting features extracted from individual frames to the trained SVMs, which are described in Section 3.4. Then the posterior probability <EM>P</EM>(<EM>x<SUB>t</SUB></EM> | <EM>Z</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM>) is propagated throughout the video using sequential Bayesian inference, i.e., Eqn (3). The probabilities <EM>P</EM>(<EM>x<SUB>t</SUB></EM> | <EM>Z</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM>) therefore describe the temporal characteristics of facial expressions in videos, and provide the quantitative measurements that our method will use for subsequent analysis. These frame-wise probabilities help create a probabilistic profile for the expression, which can be visualized as a graph (see <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F10/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 10</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f10.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f10.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 10" title="Figure 10"></SPAN></SPAN></A>) with each curve corresponding to the response to the classifier from a particular expression. The five curves together form probabilistic profiles of facial expressions in videos.</DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F10" name="F10"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F10/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f10.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f10.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 10" title="Figure 10"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f10.gif" class="icon-reflink small-thumb" alt="Figure 10" title="Figure 10"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F10/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Figure 10</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Probabilistic profiles of facial expressions in video for (a) a healthy control; (b) a patient with schizophrenia; (c) a patient with Asperger’s syndrome. From left to right, graphs in each row show the probabilities obtained from an individual’s</SPAN><A class="side-caption" style="font-size: 100%;" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F10/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"> (more ...)</A></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV></DIV><DIV id="S16" class="sec sec-last"><SPAN></SPAN><DIV class="head2 head-separate">3.6 Information Extracted from Probabilistic Profiles: Potential Relevance to Neuropsychiatric disorders</DIV><DIV class="p p-first" id="P27"><SPAN></SPAN>The probabilistic facial expression profiles provide rich information about the subtle and dynamic facial expression changes in video. Our method extracts several types of measurements from probabilistic profiles for facial expression analysis. The first measurement is the average of posterior probabilities of intended emotions, as a measurement of appropriate facial expressions. For the video segment of the <EM>i</EM>-th intended emotion (e.g., one of happiness, sadness, anger, and fear), the averaged measurement is denoted as 
<SPAN id="__inline-formulaid441224"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/M4" style="vertical-align: middle; background: #F8F8F8;padding: 2pt;border-style: none;" class="equation" alt="equation M4" title="equation M4"></SPAN>, where <EM>n<SUB>i</SUB></EM> is the length of corresponding video for the <EM>i</EM>-th intended emotion. The measurement <EM><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/xpmacr.gif" border="0" alt="P" title=""><SUB>i</SUB></EM> quantifies the correlation between participants’ facial expressions and their intended emotions. A larger <EM><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/xpmacr.gif" border="0" alt="P" title=""><SUB>i</SUB></EM> refers to a greater expression of the intended emotion and a lower value corresponds to the amount of inappropriate affect. Therefore, by comparing the measurements of individuals from different groups, we can quantify the facial expression impairments.</DIV><DIV class="p" id="P28"><SPAN></SPAN>Another measurement derived from a probabilistic facial expression profile is the probability of the neutral facial expressions in videos. For each video segment that contains one intended emotion, the posterior probability of the neutral expression indicates the lack of facial expression, and hence functions as a measure of flat affect, and can be correlated with flat affect ratings. Also, to eliminate the impact of different video lengths, we average the probability of neutral expression for each intended emotion, denoted as 
<SPAN id="__inline-formulaid441376"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/M5" style="vertical-align: middle; background: #F8F8F8;padding: 2pt;border-style: none;" class="equation" alt="equation M5" title="equation M5"></SPAN>. Thus, the probabilistic profile and the measures of flat and inappropriate affect computed from the probabilistic profile of facial expressions, quantify the two major deficits associated with neuropsychiatric disorders.</DIV><DIV class="p p-last" id="P29"><SPAN></SPAN>Except for the average probabilities, two other measurements are the occurrence frequency of the appropriate and neutral expressions. Assuming that during a video, the number of frames where the maximal poster probability corresponds to the appropriate (when the expression picked by the probabilistic classifier is same as the intended) and neutral (when the classifier identifies the expression as neutral) expressions are <EM>l<SUB>a</SUB></EM> and <EM>l<SUB>n</SUB></EM> respectively, the occurrence frequency of appropriate and neutral expressions are defined as 
<SPAN id="__inline-formulaid441517"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/M6" style="vertical-align: middle; background: #F8F8F8;padding: 2pt;border-style: none;" class="equation" alt="equation M6" title="equation M6"></SPAN>, and 
<SPAN id="__inline-formulaid441559"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/M7" style="vertical-align: middle; background: #F8F8F8;padding: 2pt;border-style: none;" class="equation" alt="equation M7" title="equation M7"></SPAN>. Based on definitions, the two measurements describe appropriateness and flatness of facial expressions. These four measures indicate that the probabilistic profile has rich information for facial expression analysis. Developing more measurements from probabilistic profiles to better describe dynamics of facial expressions remains part of future research.</DIV></DIV></DIV></DIV><DIV class="sec" id="S17"><SPAN></SPAN><DIV class="head1 section-title" id="S17titletitle" style="text-transform: none;"><DIV class="other-sections"><UL class="noext-menu"><LI><A class="first-link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#">&nbsp;Other Sections▼</A><UL class="submenu head1"><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#__abstractid436388" class="" style="text-transform: none;">Abstract</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S1" class="" style="text-transform: none;">1. Introduction</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S2" class="" style="text-transform: none;">2. Related Work</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S7" class="" style="text-transform: none;">3. Methods</A></LI><LI class="submenu-item current-item"><A class="" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#" onclick="return(false)" style="text-transform: none;">4. Results</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S24" class="" style="text-transform: none;">5. Discussion and Future Work</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#__ref-listid445193" class="" style="text-transform: none;">References</A></LI></UL></LI></UL></DIV><DIV>4. Results</DIV></DIV><DIV class="section-content" id="S17content"><DIV class="p p-first-last" id="P30"><SPAN></SPAN>In this section, we present results obtained by applying our framework to a few datasets that underline the generalization capability, ease of applicability and automated nature of our method. We first train and validate the probabilistic facial expression classifiers that are to be applied at each frame, using a dataset of actors [<A href="http://www.ncbi.nlm.nih.gov/pubmed/11992665" rid="R50" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">50</A>]. The application of our framework is also validated by comparing the classification in video with human rating results. We then apply our computational framework to a collection of video segments from healthy people and present case studies on a patient with schizophrenia and a patient with Asperger’s syndrome, which demonstrates the potential applicability of our framework.</DIV><DIV id="S18" class="sec"><SPAN></SPAN><DIV class="head2 head-separate">4.1 Validation of Probabilistic Classifiers on Actors</DIV><DIV class="p p-first" id="P31"><SPAN></SPAN>Although there are some existing facial expression databases [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R21" rid="R21" class="cite-reflink bibr popnode">21</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R51" rid="R51" class="cite-reflink bibr popnode">51</A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R52" rid="R52" class="cite-reflink bibr popnode">52</A>], none of them are designed for clinical studies, especially the study of neuropsychiatric disorders such as schizophrenia. They mainly comprise posed expressions that actually do not follow the true trend of emotions, and usually contain expressions of only high intensity. In this study, we use a database of evoked facial expression images collected from professional actors, which have been acquired under experimental conditions that are similar to our patient/control data described below at Section 4.2. The actors database contains posed and evoked expressions of 32 professional actors [<A href="http://www.ncbi.nlm.nih.gov/pubmed/11992665" rid="R50" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">50</A>]. For each type of facial expression, the actors started with a neutral face, and then were guided by professional theater directors through enactments of each of the four universal emotions of happiness, sadness, anger, and fear based on the evoked emotions procedure [<A href="http://www.ncbi.nlm.nih.gov/pubmed/11992665" rid="R50" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">50</A>]. Images were acquired while these actors were expressing emotion at three levels of intensity: mild, medium, and peak. Selected face examples are shown in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F8/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Fig. 8</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f8.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f8.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 8" title="Figure 8"></SPAN></SPAN></A>.</DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F8" name="F8"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F8/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f8.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f8.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 8" title="Figure 8"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f8.gif" class="icon-reflink small-thumb" alt="Figure 8" title="Figure 8"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F8/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Figure 8</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Emotional expressions from the professional actor database (a) neutral; (b) happiness; (c) sadness; (d) anger; (e) fear</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="p" id="P32"><SPAN></SPAN>The dataset is used as the training and validation data for facial expression classifiers. While posed databases have been used in the past for many expression studies, there is evidence that evoked emotions are more accurately perceived than posed expressions [<A href="http://www.ncbi.nlm.nih.gov/pubmed/11992665" rid="R50" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">50</A>], and therefore we only use the evoked expressions in this study. The training images include four expressions (i.e., happiness, sadness, anger, fear) at all intensities of facial expression and neutral expression. We apply the method described in Section 3, except for the tracking part, on actors’ images to create facial expression classifiers. The landmarks are detected on these facial images, and features are then extracted from these landmarks, as explained in Section 3. Using extracted features, total five SVM classifiers are trained, with one for each of the four expressions and the neutral expression. In order to test the accuracy of trained classifiers, they are further validated through a cross-validation procedure that is explained as follows. In each iteration of the cross-validation, face images from one subject are left out from the training data (neutral faces as well as faces with expression), and are tested on the classifiers trained on the remaining samples. The validation iterates until all the subjects are left out once and only once for testing. The testing accuracy averaged over all the data indicates the accuracy of trained classifiers. <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T1/" style="text-decoration:none;" onclick="startTarget(this, &#39;true&#39;, 1024, 800)" class="fig-table-link table"><SPAN style="text-decoration: underline;">Table 1</SPAN></A> summarizes the cross-validation accuracy of the facial expression classifiers. In this table, the rows show intended emotions, which are considered as ground truth in this validation, and the columns show classified expressions.</DIV><DIV class="canvas-table-ref-outer"><DIV class="canvas-table-ref-inner"><A id="T1" name="T1"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T1/" onclick="startTarget(this, &#39;table&#39;, 1024, 800)"><DIV class="thumb-ph"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/table-icon.gif" class="icon-reflink" style="border: 1px solid;" alt="Table 1" title="Table 1"></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T1/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Table 1</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Confusion matrix from the cross-validation on actors’ data</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="p p-last" id="P33"><SPAN></SPAN>Our validation is further compared with human rating results. In a previous study [<A href="http://www.ncbi.nlm.nih.gov/pubmed/11992665" rid="R50" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">50</A>], 41 students from undergraduate and graduate courses in psychology at the Drexel University were recruited as human raters. The raters were shown each face, and were asked to identify the emotional content of the face. The human raters were able to correctly identify 98% correct for happiness, and 67% correct for sadness, 77% correct for anger, 67% correct for fear. The overall accuracy of human raters is 77.8%, which is comparable with our cross validation accuracy. With more control/patient data being collected in our study, our ultimate goal is to use controls’ data as the ground truth to train the facial expression classifiers.</DIV></DIV><DIV id="S19" class="sec sec-last"><SPAN></SPAN><DIV class="head2 head-separate">4.2 Preliminary Results on Control/Patient Data</DIV><DIV id="S20" class="sec sec-first"><SPAN></SPAN><SPAN class="head3">4.2.1 Data Collection </SPAN> <DIV class="p p-first-last" id="P34"><SPAN></SPAN>In our preliminary study, facial expressions of individuals from different groups, including healthy controls, patients with schizophrenia, and patients with Asperger’s syndrome, are acquired under the supervision of psychiatrists. The data was acquired under an approved IRB protocol of the University of Pennsylvania and permission has been obtained from subjects for the publication of pictures. All the participants are chosen in pairs matched for age, ethnicity, and gender. Each participant undergoes two conditions: posed and evoked. In the posed session, participants are asked to express the emotions of happiness, anger, fear, sadness and disgust, at mild, medium, and peak levels. In the evoked session, participants are individually guided through vignettes, which are provided by participants themselves, and describe a situation in their life pertaining to each emotion. In order to elicit evoked expressions, the vignettes are recounted back to the participants by a psychiatrist, who guides them through all the three levels of expression intensity (mild, medium and peak) for each emotion. The videos and 2D images are acquired during the course of the expression using the setup illustrated in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F9/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 9</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f9.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f9.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 9" title="Figure 9"></SPAN></SPAN></A>. There are six grayscale stereo camera, one color camera and a video camera [<A href="http://www.ncbi.nlm.nih.gov/pubmed/15585289" rid="R14" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">14</A>]. The color camera captures the 2D images, work on which has been described in [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R15" rid="R15" class="cite-reflink bibr popnode">15</A>]. The six grayscale cameras and the color camera are calibrated to produce images that are used for 3D reconstruction of the faces. Our work using 3D surface data to analyze facial expressions is beyond the scope of this paper. Since evoked emotions are more accurately perceived than posed expressions, we only use the videos of evoked expressions for facial expression analysis, by applying the presented framework. We also exclude disgust from the analysis. Video recordings of facial emotional expression are segmented into 5 clips, 1 for each of the five emotions expressed. Each emotional segment begins from the mild intensity expressed, and ends at the extreme intensity, as identified during interview. The patient/control database is currently small and hence we use a few of the datasets to demonstrate the applicability of our framework. In future, as the dataset grows, we will be able to perform a group-based analysis, using the probabilistic profiles for the expressions obtained from our framework, via measures of flat and inappropriate affect computed from these.</DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F9" name="F9"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F9/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f9.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f9.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 9" title="Figure 9"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f9.gif" class="icon-reflink small-thumb" alt="Figure 9" title="Figure 9"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F9/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Figure 9</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>The data capturing system</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV></DIV><DIV id="S21" class="sec sec-last"><SPAN></SPAN><SPAN class="head3">4.2.2 Application of the Video Based Expression Analysis Framework </SPAN> <DIV class="p p-first-last" id="P35"><SPAN></SPAN>The method described in Section 3 is applied to several video clips, each of which contains one type of facial emotional expression of a participant, to obtain the probabilistic profile for facial expression. First, landmarks are detected and tracked in the video, and then facial expression features are extracted from tracked landmarks. The extracted features are input to facial expression classifiers that have been trained using actors’ data, to obtain posterior probabilities of facial expression in videos. In order to validate our framework, we first compare our method with the human ratings using the Facial Expression Coding System (FACES) [<A href="http://www.ncbi.nlm.nih.gov/pubmed/17563202" rid="R7" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">7</A>] on a healthy control group. To further demonstrate its applicability to patients, we examine the method on healthy controls, a patient with schizophrenia, and a patient with Asperger’s syndrome.</DIV><DIV id="S22" class="sec"><SPAN></SPAN><SPAN class="head4">4.2.2.1 Validation on FACES </SPAN> <DIV class="p p-first" id="P36"><SPAN></SPAN>In order to validate the video based framework, we compare our results with human ratings from Facial Expression Coding System (FACES) performed by human raters, on facial expressions from the healthy control group. In FACES, facial expressions in video segments are coded for frequency, duration, valance (positive or negative), and intensity (low, medium, high, very high). Two trained raters coded the frequency of facial expressions in each video segment. Expressions were coded if a neutral expression changed to an emotional expression and changed back to a neutral expression (1 expression coded) or to a different emotional expression (2 expressions coded). Facial changes independent of emotion expression (e.g. yawning, licking lips, talking, head nodding, head tilt, diverted eye gaze) were not counted as an emotion expression. For every expression, the emotion (happiness, sadness, anger, and fear), intensity (3-point scale of mild, moderate, and extreme), and duration (in seconds) were coded.</DIV><DIV class="p" id="P37"><SPAN></SPAN>For each intended emotion, i.e., one of happiness, sadness, anger and fear, we have used 9 videos of healthy controls. For a rater to perform human FACES ratings, a video clip acquired from each participant is divided into separate segments, with each segment only corresponding to one type of intended emotion. All the segments are randomized such that raters were blind to the participants’ intended emotion. With capturing speed at 15 frames per second, the length of segmented videos in the control group varies between 646 and 1431 frames for happiness, between 815 and 2620 frames for sadness, between 680 and 3252 frames for anger and between 1042 and 2578 frames for fear. The two raters are consistent with each other at most cases, even they may have small disagreements in the beginning and ending time of each segmented expression. FACES rating results from both raters are summarized in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T2/" style="text-decoration:none;" onclick="startTarget(this, &#39;true&#39;, 1024, 800)" class="fig-table-link table"><SPAN style="text-decoration: underline;">Table 2</SPAN></A>. In the table, rows show intended emotions of video segments, and columns show their FACES ratings.</DIV><DIV class="canvas-table-ref-outer"><DIV class="canvas-table-ref-inner"><A id="T2" name="T2"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T2/" onclick="startTarget(this, &#39;table&#39;, 1024, 800)"><DIV class="thumb-ph"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/table-icon.gif" class="icon-reflink" style="border: 1px solid;" alt="Table 2" title="Table 2"></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T2/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Table 2</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Confusion matrix of FACES ratings vs. intended emotions of controls</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="p" id="P38"><SPAN></SPAN>Since there are possible inconsistency between human ratings and the intended emotion of participants, there are two types of percentages, shown in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T2/" style="text-decoration:none;" onclick="startTarget(this, &#39;true&#39;, 1024, 800)" class="fig-table-link table"><SPAN style="text-decoration: underline;">Table 2</SPAN></A>, to interpret the FACES rating results. Type I percentage refers to, among all the videos captured in an intended emotion session, the percentage of expressions that are rated as the corresponding intended emotion. For example, among all video segments captured as part of the session for intended happiness, there are 82.0% expressions are rated as happiness by raters based on the FACES rule, and the remaining are rated as other expressions. The type II percentage illustrates, among all the expressions rated from FACES, the percentage of expressions that are actually from the corresponding intended emotion sessions. For example, among all the video segments rated as anger by FACES, 70.0% are from the intended anger sessions. The type I percentage is low for sadness, anger, and fear, demonstrating that videos may contain other expressions during one session of single intended emotion. The type II percentages show that the expression of happiness and sadness can appear in other emotion sessions, while anger and fear expressions appear more in the corresponding emotion sessions. Low percentages of both types demonstrate the uncertainty in expression and perception of emotions, and also highlight the difficulties of automatic analysis of evoked and subtle emotions.</DIV><DIV class="p p-last" id="P39"><SPAN></SPAN>We further compare our results from probabilistic profiles with human ratings from FACES, and show that our automatic method presents a reasonable accuracy. In this experiment, we validate only on those expressions in which the FACES ratings are consistent with intended emotions, to reduce the uncertainty factor in human emotion ratings. After generating probabilistic profiles, the mean posterior probabilities of each emotion in videos, i.e, <EM><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/xpmacr.gif" border="0" alt="P" title=""><SUB>i</SUB></EM>, are used for facial expression recognition. The expression corresponding to the largest <EM><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/xpmacr.gif" border="0" alt="P" title=""><SUB>i</SUB></EM> is considered the intended emotion in the video. <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T3/" style="text-decoration:none;" onclick="startTarget(this, &#39;true&#39;, 1024, 800)" class="fig-table-link table"><SPAN style="text-decoration: underline;">Table 3</SPAN></A> summarizes the comparison results between FACES rating and our classification results. The rows show expressions rated from FACES, and the columns show automatically classified expressions based on the principle of maximal posterior probability. The recognition, except for the expression of sadness, provides reasonable results. Since all the classification results are based on the classifiers trained using actors’ data, as we currently do not have enough controls for both training and validation, we expect that the accuracy would be increased when we have enough controls for training.</DIV><DIV class="canvas-table-ref-outer"><DIV class="canvas-table-ref-inner"><A id="T3" name="T3"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T3/" onclick="startTarget(this, &#39;table&#39;, 1024, 800)"><DIV class="thumb-ph"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/table-icon.gif" class="icon-reflink" style="border: 1px solid;" alt="Table 3" title="Table 3"></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T3/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Table 3</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Confusion matrix of classified expressions vs. FACES ratings of controls</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE></DIV></DIV><DIV style="clear:both;"></DIV></DIV><DIV id="S23" class="sec sec-last"><SPAN></SPAN><SPAN class="head4">4.2.2.2 Case Studies on Individuals from Different Groups </SPAN> <DIV class="p p-first" id="P40"><SPAN></SPAN>The measurements extracted from probabilistic profiles can be used to examine different groups, such as healthy controls, and patients with deficits in facial expressions. Here we demonstrate the scalability of our method by applying it on three individuals, one from each group: healthy controls, patients with schizophrenia, and patients with Asperger’s syndrome.</DIV><DIV class="p" id="P41"><SPAN></SPAN><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F10/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 10</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f10.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f10.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 10" title="Figure 10"></SPAN></SPAN></A> shows the visualization of facial expressions probabilistic profiles as graphs of posterior probabilities of four facial expressions and neutral faces in each intended emotion of the three participants. In this figure, each color represents one of the four emotions: happiness (green), sadness (blue), anger (red), fear (yellow), and the neutral expression (brown). The horizontal axis represents the frame index, and the vertical axis represents the posterior probabilities <EM>P</EM>(<EM>x<SUB>t</SUB></EM> | <EM>Z</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM>) for one of four emotions and neutral expression, which is denoted as <EM>x<SUB>t</SUB></EM>, at the <EM>t</EM>-th frames. Some frames from the videos corresponding to these profiles with the corresponding probabilities are shown in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F11/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure (11)</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f11.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f11.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 11" title="Figure 11"></SPAN></SPAN></A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F12/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">(12)</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f12.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f12.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 12" title="Figure 12"></SPAN></SPAN></A> and <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F13/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">(13)</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f13.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f13.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 13" title="Figure 13"></SPAN></SPAN></A>, where the probabilities are visualized as bars on the top right corner, with the bar of the longest length corresponding to the outcome of the frame on the application of the classifiers. As displayed in these figures, the posterior probabilities of expressions, <EM>P</EM>(<EM>x<SUB>t</SUB></EM> | <EM>Z</EM><SUB>1:</SUB><EM><SUB>t</SUB></EM>), indicate the trends of facial expression changes of individuals in the video.</DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F11" name="F11"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F11/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f11.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f11.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 11" title="Figure 11"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f11.gif" class="icon-reflink small-thumb" alt="Figure 11" title="Figure 11"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F11/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Figure 11</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Emotional expressions of a healthy control: (a) happiness; (b) anger; (c) fear. The length of the bar is proportional to the probability associate with each expression. The original probability scales between 0 and 1</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F12" name="F12"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F12/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f12.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f12.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 12" title="Figure 12"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f12.gif" class="icon-reflink small-thumb" alt="Figure 12" title="Figure 12"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F12/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Figure 12</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Emotional expressions of a patient with schizophrenia (a) happiness; (b) anger; (c) fear. The length of the bar is proportional to the probability associate with each expression</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="canvas-figure-ref-outer"><DIV class="canvas-figure-ref-inner"><A id="F13" name="F13"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F13/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="icon-reflink figpopup"><DIV class="thumb-ph"><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f13.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f13.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 13" title="Figure 13"></SPAN></SPAN><DIV class="small-thumb-canvas"><DIV class="small-thumb-canvas-1"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f13.gif" class="icon-reflink small-thumb" alt="Figure 13" title="Figure 13"></DIV></DIV></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F13/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Figure 13</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Emotional expressions of a patient with Asperger’s syndrome: (a) happiness; (b) anger; (c) fear. The length of the bar is proportional to the probability associate with each expression</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE><DIV class="figure-table-caption-in-article"></DIV></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="p" id="P42"><SPAN></SPAN>An inspection of <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F10/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figures 10</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f10.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f10.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 10" title="Figure 10"></SPAN></SPAN></A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F11/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="position: relative;text-decoration:none;">​<SPAN class="figpopup-sensitive-area" style="left: -0.5em;">,11,</SPAN></SPAN><SPAN style="text-decoration: underline;">11</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f11.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f11.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 11" title="Figure 11"></SPAN></SPAN></A>, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F12/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="position: relative;text-decoration:none;">​<SPAN class="figpopup-sensitive-area" style="left: -0.5em;">,12</SPAN></SPAN><SPAN style="text-decoration: underline;">12</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f12.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f12.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 12" title="Figure 12"></SPAN></SPAN></A> and <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F13/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="position: relative;text-decoration:none;">​<SPAN class="figpopup-sensitive-area" style="left: -1.5em;">and13</SPAN></SPAN><SPAN style="text-decoration: underline;">13</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f13.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f13.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 13" title="Figure 13"></SPAN></SPAN></A> indicates that probabilistic profiles of facial expressions are able to capture subtle expressions and to identify expressions that are different from the intended emotion, hence determining the inappropriateness of emotion, as well as identify frames that have neutral expression thereby identifying the flatness of expression. The probability bars associated with the top right corner reveals that the classifier is able to correctly determine the type and intensity of emotion. In <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F11/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Fig 11(c)</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f11.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f11.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 11" title="Figure 11"></SPAN></SPAN></A>, neutral is picked up instead of fear. The classifier is able to identify the emotion correctly even when the expression deviated from the intended (<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F12/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Fig. 12(b)</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f12.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f12.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 12" title="Figure 12"></SPAN></SPAN></A>, frame 1, sadness is identified instead of the intended anger and in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F13/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">fig. 13(c)</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f13.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f13.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 13" title="Figure 13"></SPAN></SPAN></A> in which sadness is identified instead of intended fear). These expressions are rated to be correct by a human rater. Subtle expressions are also well identified (<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F13/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Fig. 13(a)</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f13.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f13.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 13" title="Figure 13"></SPAN></SPAN></A>, frame 3, <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F12/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Fig. 12(a)</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f12.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f12.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 12" title="Figure 12"></SPAN></SPAN></A>, frame 3).</DIV><DIV class="p" id="P43"><SPAN></SPAN>After obtaining the probabilistic profiles of facial expression for each intended emotion, we compute quantitative measurements to characterize facial expressions in video. As described in Section 3.6, four types of measurements are calculated, i.e., <EM><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/xpmacr.gif" border="0" alt="P" title=""><SUB>i</SUB></EM>, <EM><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/xnmacr.gif" border="0" alt="N" title=""><SUB>i</SUB></EM>, <EM>f<SUB>a</SUB></EM>, and <EM>f<SUB>n</SUB></EM>. Specifically, <EM><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/xpmacr.gif" border="0" alt="P" title=""><SUB>i</SUB></EM> and <EM>f<SUB>a</SUB></EM> quantifies the appropriate expression for the <EM>i</EM>-th intended emotion (e.g., one of the four emotions: happiness, sadness, anger, fear), and <EM><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/xnmacr.gif" border="0" alt="N" title=""><SUB>i</SUB></EM> and <EM>f<SUB>n</SUB></EM> quantify the neutral expression in the <EM>i</EM>-th intended emotion. These measurements will be used to correlate with clinical measurements of inappropriate and flat affect when we have collected enough samples for the group study. <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T4/" style="text-decoration:none;" onclick="startTarget(this, &#39;true&#39;, 1024, 800)" class="fig-table-link table"><SPAN style="text-decoration: underline;">Table 4</SPAN></A> and <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T5/" style="text-decoration:none;" onclick="startTarget(this, &#39;true&#39;, 1024, 800)" class="fig-table-link table"><SPAN style="text-decoration: underline;">5</SPAN></A> show two types of average probabilities for each emotion, along with the averages over all the emotions, for the three participants. <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T6/" style="text-decoration:none;" onclick="startTarget(this, &#39;true&#39;, 1024, 800)" class="fig-table-link table"><SPAN style="text-decoration: underline;">Table 6</SPAN></A> and <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T7/" style="text-decoration:none;" onclick="startTarget(this, &#39;true&#39;, 1024, 800)" class="fig-table-link table"><SPAN style="text-decoration: underline;">7</SPAN></A> show two occurrence frequency measurements for each emotion.</DIV><DIV class="canvas-table-ref-outer"><DIV class="canvas-table-ref-inner"><A id="T4" name="T4"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T4/" onclick="startTarget(this, &#39;table&#39;, 1024, 800)"><DIV class="thumb-ph"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/table-icon.gif" class="icon-reflink" style="border: 1px solid;" alt="Table 4" title="Table 4"></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T4/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Table 4</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Average probability of intended emotion in videos of three participants</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="canvas-table-ref-outer"><DIV class="canvas-table-ref-inner"><A id="T5" name="T5"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T5/" onclick="startTarget(this, &#39;table&#39;, 1024, 800)"><DIV class="thumb-ph"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/table-icon.gif" class="icon-reflink" style="border: 1px solid;" alt="Table 5" title="Table 5"></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T5/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Table 5</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Average probability of neutral expression in videos of three participants</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="canvas-table-ref-outer"><DIV class="canvas-table-ref-inner"><A id="T6" name="T6"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T6/" onclick="startTarget(this, &#39;table&#39;, 1024, 800)"><DIV class="thumb-ph"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/table-icon.gif" class="icon-reflink" style="border: 1px solid;" alt="Table 6" title="Table 6"></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T6/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Table 6</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Occurrence frequency of appropriate expressions in videos of three participants</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="canvas-table-ref-outer"><DIV class="canvas-table-ref-inner"><A id="T7" name="T7"></A><TABLE class="thumb-caption" border="0" cellpadding="0" cellspacing="0" style="clear:both; width: 100%;"><TBODY><TR valign="top" align="left"><TD class="thumb-cell"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T7/" onclick="startTarget(this, &#39;table&#39;, 1024, 800)"><DIV class="thumb-ph"><IMG src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/table-icon.gif" class="icon-reflink" style="border: 1px solid;" alt="Table 7" title="Table 7"></DIV></A></TD><TD class="caption-cell"><DIV class="caption-ph"><A class="side-caption" href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T7/" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)"><STRONG>Table 7</STRONG></A><DIV class="figure-table-caption-in-article"><SPAN>Occurrence frequency of neutral expression in videos of three participants</SPAN></DIV><DIV class="figure-table-caption-in-article"></DIV></DIV></TD></TR></TBODY></TABLE></DIV></DIV><DIV style="clear:both;"></DIV><DIV class="p p-last" id="P44"><SPAN></SPAN><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T4/" style="text-decoration:none;" onclick="startTarget(this, &#39;true&#39;, 1024, 800)" class="fig-table-link table"><SPAN style="text-decoration: underline;">Table 4</SPAN></A> and <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T6/" style="text-decoration:none;" onclick="startTarget(this, &#39;true&#39;, 1024, 800)" class="fig-table-link table"><SPAN style="text-decoration: underline;">6</SPAN></A> demonstrate that overall, the healthy control expresses intended emotion better than the patient with Asperger’s and schizophrenia (especially in the fear). The averages (column 6) in both the tables show that the levels of impairment of the Asperger’s patient lie in between that of the controls and the schizophrenia patient. <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T5/" style="text-decoration:none;" onclick="startTarget(this, &#39;true&#39;, 1024, 800)" class="fig-table-link table"><SPAN style="text-decoration: underline;">Table 5</SPAN></A> and <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/table/T7/" style="text-decoration:none;" onclick="startTarget(this, &#39;true&#39;, 1024, 800)" class="fig-table-link table"><SPAN style="text-decoration: underline;">7</SPAN></A> also shows that the individuals demonstrate different levels of expressiveness. However, the control has more neutral expression than the two patients. As confirmed by clinical ratings (using SANS [<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#R6" rid="R6" class="cite-reflink bibr popnode">6</A>]) by two experts, the controls actually show almost the same level of flatness as the patients (the flatness index scores at 2 and 3 according to two raters). However, such an observation does not permit conclusions regarding group behavior of patients relative to controls. We expect to perform group difference studies when more patient data has been acquired.</DIV></DIV></DIV></DIV></DIV></DIV><DIV class="sec" id="S24"><SPAN></SPAN><DIV class="head1 section-title" id="S24titletitle" style="text-transform: none;"><DIV class="other-sections"><UL class="noext-menu"><LI><A class="first-link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#">&nbsp;Other Sections▼</A><UL class="submenu head1"><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#__abstractid436388" class="" style="text-transform: none;">Abstract</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S1" class="" style="text-transform: none;">1. Introduction</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S2" class="" style="text-transform: none;">2. Related Work</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S7" class="" style="text-transform: none;">3. Methods</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S17" class="" style="text-transform: none;">4. Results</A></LI><LI class="submenu-item current-item"><A class="" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#" onclick="return(false)" style="text-transform: none;">5. Discussion and Future Work</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#__ref-listid445193" class="" style="text-transform: none;">References</A></LI></UL></LI></UL></DIV><DIV>5. Discussion and Future Work</DIV></DIV><DIV class="section-content" id="S24content"><DIV class="p p-first" id="P45"><SPAN></SPAN>In this paper, we present an automated computational framework for analyzing facial expressions using video data, producing a probabilistic profile of expression change. The framework explores rich information contained in the video, by providing a probabilistic composition of each frame of the sequence, thereby highlighting subtle differences as well as the possibility of a mixture of emotions. The potential relevance for neuropsychiatric disorders stems from the propensity for impaired emotion expression including inappropriate or flat affect. Thus far diagnosis of impaired affect expression required trained clinical observers. The framework benefits from being automated, thereby helping in processing lengthy video sequences. It is also applicable to participants from groups with different pathologies or various stages of disease progression.</DIV><DIV class="p" id="P46"><SPAN></SPAN>The preliminary results demonstrate the capability of our video-based expression analysis method in identifying characteristics of facial expressions through probabilistic expression profiles (<A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F10/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 10</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f10.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f10.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 10" title="Figure 10"></SPAN></SPAN></A>). These expression profiles, in conjunction with the metrics of appropriateness and flatness computed from them, provide extensive information about the expression and capture the subtleties of expression change. Patients follow different trends of facial expression than healthy participants. The facial expressions of the healthy control are more consistent with the expected trend of intended emotion, that is the emotion gradually progresses from mild to moderate and finally to the peak level. Especially for the expressions of anger and fear, the facial expression trends of the healthy control better characterize the intended emotion than the patients. Another observation is that the intensity of expression of the healthy control is higher than the patients. The differences between the three subjects are mainly in the negative emotions of sadness and fear. Especially for fear, the healthy control is more expressive than the two patients. Also the measurements averaged over all expressions demonstrate the difference between individuals, although the differences in the happy and anger expressions are small. We believe that with additional enrollment of subjects, our framework will be able to identify significant group differences using the presented computational methods.</DIV><DIV class="p" id="P47"><SPAN></SPAN>It is also observed that the facial expression recognition results of the expression of sadness (as seen in the graph of probabilities in <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/mid/NIHMS39199/figure/F10/" style="text-decoration:none;" onclick="startTarget(this, &#39;figure&#39;, 1024, 800)" class="fig-table-link fig figpopup"><SPAN style="text-decoration: underline;">Figure 10</SPAN><SPAN class="large-thumb-canvas"><SPAN class="large-thumb-canvas-1"><IMG hires="/pmc/articles/PMC2238802/bin/nihms39199f10.jpg" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/nihms39199f10.gif" style="border: 1px solid" class="icon-reflink large-thumb" alt="Figure 10" title="Figure 10"></SPAN></SPAN></A>) are not as good as other facial expressions. Sadness is somehow confused with anger expression perhaps owing to the following two reasons. First, the sad and anger expressions share some similar facial movements, such as eyebrow lower, and lip corner depressor [<A href="http://www.ncbi.nlm.nih.gov/pubmed/15541780" rid="R53" class="cite-reflink bibr popnode" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CBody&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">53</A>]. Such facial movements may cause confusion between two expressions. Second, the participants (both patients and controls) usually show more subtle expressions than actors. Since our classifiers are built on actors’ expressions, they may not recognize well the low intensity expression of sadness and concentrate more on salient facial expressions such as anger. Our solution to this problem is to retrain the facial expression classifiers using data from healthy controls when additional data from healthy controls is available. We believe that by training facial expression classifiers based on a healthy population, our method can better characterize the true trends of intended emotions. We expect that training with healthy controls will also help the separation between sadness and anger.</DIV><DIV class="p p-last" id="P48"><SPAN></SPAN>The experiments pave the way for establishing a video based method for quantitative analysis of facial expressions in clinical research. The method can be applied to any disorder that causes affect deficits. The probabilistic profile of facial expressions provides a graphical visualization of affect deficits as well as measures to quantify flatness and inappropriateness of expression. In future, we will apply our framework to large population group-based studies, to quantify the group differences between healthy controls and patients, to correlate with clinical measurements, and to obtain a population profile of temporal change during the course of a facial expression. We expect that the knowledge obtained from such an analysis will help in diagnosis, prognosis, and studying treatment effects.</DIV></DIV></DIV><DIV class="sec" id="S26"><SPAN></SPAN><DIV class="head1 section-title" id="S26titletitle" style="text-transform: none;"><DIV>Acknowledgments</DIV></DIV><DIV class="section-content" id="S26content"><DIV class="sec"><DIV class="p" id="__pid445190">We would like to acknowledge support from NIH grants 1R01MH73174-01 (for method development) and R01-MH060722 (for data acquisition).</DIV></DIV></DIV></DIV><DIV class="sec" id="__articleid570314footnotes"><DIV class="head1 section-title" id="__articleid570314footnotestitletitle" style="text-transform: none;"><DIV>Footnotes</DIV></DIV><DIV class="section-content" id="__articleid570314footnotescontent"><DIV class="fm-footnote" id="FN2"><DIV class="p p-first-last" id="__pid447686"><STRONG>Publisher's Disclaimer: </STRONG>This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</DIV></DIV></DIV></DIV><DIV class="sec" id="__ref-listid445193"><DIV class="head1 section-title" id="__ref-listid445193titletitle" style="text-transform: none;"><DIV class="other-sections"><UL class="noext-menu"><LI><A class="first-link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#">&nbsp;Other Sections▼</A><UL class="submenu head1"><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#__abstractid436388" class="" style="text-transform: none;">Abstract</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S1" class="" style="text-transform: none;">1. Introduction</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S2" class="" style="text-transform: none;">2. Related Work</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S7" class="" style="text-transform: none;">3. Methods</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S17" class="" style="text-transform: none;">4. Results</A></LI><LI class="submenu-item"><A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#S24" class="" style="text-transform: none;">5. Discussion and Future Work</A></LI><LI class="submenu-item current-item"><A class="" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#" onclick="return(false)" style="text-transform: none;">References</A></LI></UL></LI></UL></DIV><DIV>References</DIV></DIV><DIV class="section-content" id="__ref-listid445193content"><DIV class="back-matter-section" id="reference-list"><DIV class="ref-cit-blk" id="R1"><SPAN class="ref-label">1.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445202">Morrison RL, Bellack AS, Mueser KT. Deficits in facial-affect recognition and schizophrenia. <SPAN><SPAN class="ref-journal">Schizophrenia Bulletin. </SPAN>1988;<SPAN class="ref-vol">14</SPAN>(1):67–83.</SPAN>  <SPAN style="white-space: nowrap;">[<A class="ref-extlink" href="http://www.ncbi.nlm.nih.gov/pubmed/3291095" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">PubMed</A>]</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R2"><SPAN class="ref-label">2.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445261">Berenbaum H, Oltmann TF. Emotional experience and expression in schizophrenia and depression. <SPAN><SPAN class="ref-journal">Journal of Abnormal Psychiatry Research. </SPAN>1992;<SPAN class="ref-vol">101</SPAN>(1):37–44.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R3"><SPAN class="ref-label">3.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445309">Kring AM, Neale MAJM, Harvey PD. A multichannel, multimethod assessment of affective flattening in schizophrenia. <SPAN><SPAN class="ref-journal">Psychiatry Research. </SPAN>1994;<SPAN class="ref-vol">54</SPAN>:211–222.</SPAN>  <SPAN style="white-space: nowrap;">[<A class="ref-extlink" href="http://www.ncbi.nlm.nih.gov/pubmed/7761554" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">PubMed</A>]</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R4"><SPAN class="ref-label">4.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445366">Mandal M, Pandey R, Prasad A. Facial expressions of emotions and schizophrenia: a review. <SPAN><SPAN class="ref-journal">Schizophrenia Bulletin. </SPAN>1998;<SPAN class="ref-vol">24</SPAN>(3):399–412.</SPAN>  <SPAN style="white-space: nowrap;">[<A class="ref-extlink" href="http://www.ncbi.nlm.nih.gov/pubmed/9718632" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">PubMed</A>]</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R5"><SPAN class="ref-label">5.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445426">Gur RE, et al.  Flat affect in Schizophrenia: relation to emotion processing and neurocognitive measures. <SPAN><SPAN class="ref-journal">Schizophrenia Bulletin. </SPAN>2006;<SPAN class="ref-vol">32</SPAN>(2):279–287.</SPAN>  <SPAN style="white-space: nowrap;">[<A class="ref-extlink" href="http://www.ncbi.nlm.nih.gov/pubmed/16452608" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">PubMed</A>]</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R6"><SPAN class="ref-label">6.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445474">Andreasen NC. <SPAN class="ref-journal">Scale for the Assessment of Negative Symptoms (SANS).</SPAN> Iowa City: University of Iowa; 1984. </SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R7"><SPAN class="ref-label">7.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445507">Kring AM, Sloan DS. The facial expression coding system (FACES): Development, validation, and utility. <SPAN><SPAN class="ref-journal">Psychological Assessment. </SPAN>2007;<SPAN class="ref-vol">19</SPAN>:210–224.</SPAN>  <SPAN style="white-space: nowrap;">[<A class="ref-extlink" href="http://www.ncbi.nlm.nih.gov/pubmed/17563202" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">PubMed</A>]</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R8"><SPAN class="ref-label">8.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445557">Gaebel W, Wölwer WC. Facial expression and emotional face recognition in schizophrenia and depression. <SPAN><SPAN class="ref-journal">Eur Arch Psychiatry Clin Neurosci. </SPAN>1992;<SPAN class="ref-vol">242</SPAN>:46–52.</SPAN>  <SPAN style="white-space: nowrap;">[<A class="ref-extlink" href="http://www.ncbi.nlm.nih.gov/pubmed/1390955" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">PubMed</A>]</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R9"><SPAN class="ref-label">9.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445608">Hellewell J, Connell J, Deakin JFW. Affect-judgment and facial recognition memory in schizophrenia. <SPAN><SPAN class="ref-journal">Psychopathology. </SPAN>1994;<SPAN class="ref-vol">27</SPAN>:255–261.</SPAN>  <SPAN style="white-space: nowrap;">[<A class="ref-extlink" href="http://www.ncbi.nlm.nih.gov/pubmed/7846247" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">PubMed</A>]</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R10"><SPAN class="ref-label">10.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445665">Schneider FHH, Himer W, Huss D, Mattes R, Adam B. Computer-based analysis of facial action in schizophrenic and depressed patients. <SPAN><SPAN class="ref-journal">Eur Arch Psychiatry Clin Neurosci. </SPAN>1990;<SPAN class="ref-vol">240</SPAN>(2):67–76.</SPAN>  <SPAN style="white-space: nowrap;">[<A class="ref-extlink" href="http://www.ncbi.nlm.nih.gov/pubmed/2149651" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">PubMed</A>]</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R11"><SPAN class="ref-label">11.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445738">Pantic M, Rothkrantz LJM. Automatic analysis of facial expressions: the state of the art. <SPAN><SPAN class="ref-journal">IEEE Trans on PAMI. </SPAN>2000;<SPAN class="ref-vol">22</SPAN>(12):1424–1445.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R12"><SPAN class="ref-label">12.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445787">Fasel B, Luettin J. Automatic facial expression analysis: a survey. <SPAN><SPAN class="ref-journal">Pattern Recognition. </SPAN>2003;<SPAN class="ref-vol">36</SPAN>(1):259–275.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R13"><SPAN class="ref-label">13.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445835">Tian Y-L, Kanade T, Cohn JF. <SPAN class="ref-journal">Handbook of Face Recognition.</SPAN> Springer; 2005. </SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R14"><SPAN class="ref-label">14.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445878">Verma R, et al.  Quantification of facial expressions using high dimensional shape transformations. <SPAN><SPAN class="ref-journal">Journal of Neuroscience Methods. </SPAN>2005;<SPAN class="ref-vol">141</SPAN>:61–73.</SPAN>  <SPAN style="white-space: nowrap;">[<A class="ref-extlink" href="http://www.ncbi.nlm.nih.gov/pubmed/15585289" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">PubMed</A>]</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R15"><SPAN class="ref-label">15.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445923">Christopher Alvino CK, Barrett Frederick, Gur Raquel E, Gur Ruben C, Verma Ragini. <SPAN class="ref-journal">Journal of Neuroscience Methods.</SPAN> 2007. Computerized Measurement of Facial Expression of Emotions in Schizophrenia.</SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R16"><SPAN class="ref-label">16.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid445981">Ekman P, Friesen WV. <SPAN class="ref-journal">Facial action coding system: A technique for the measurement of facial movement.</SPAN> Palo Alto, Calif.: Consulting Psychologists Press; 1978. </SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R17"><SPAN class="ref-label">17.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446021">Tian YL, Kanade T, Cohn JF. Recognizing Action Units for Facial Expression Analysis. <SPAN><SPAN class="ref-journal">IEEE Trans on PAMI. </SPAN>2001;<SPAN class="ref-vol">23</SPAN>(2):97–115.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R18"><SPAN class="ref-label">18.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446075">Bartlet MS, et al.  Measuring facial expressions by computer image analysis. <SPAN><SPAN class="ref-journal">Psychophysiology. </SPAN>1999;<SPAN class="ref-vol">36</SPAN>:253–263.</SPAN>  <SPAN style="white-space: nowrap;">[<A class="ref-extlink" href="http://www.ncbi.nlm.nih.gov/pubmed/10194972" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">PubMed</A>]</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R19"><SPAN class="ref-label">19.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446120">Fasel B, Luettin J. <SPAN class="ref-journal">ICPR.</SPAN> 2000. Recognition of asymmetric facial action unit activities and intensities; pp. 1100–1103.</SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R20"><SPAN class="ref-label">20.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446162">Lien JJ, et al.  Subtly different facial expression recognition and expression intensity estimation. <SPAN><SPAN class="ref-journal">IEEE Conference on Computer Vision and Pattern Recognition. </SPAN>1998:853–859.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R21"><SPAN class="ref-label">21.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446200">Lyons MJ, Budynek J, Akamatsu S. Automatic Classification of Single Facial Images. <SPAN><SPAN class="ref-journal">IEEE Trans on PAMI. </SPAN>1999;<SPAN class="ref-vol">21</SPAN>(12):1357–1362.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R22"><SPAN class="ref-label">22.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446255">Littlewort G, et al.  Dynamics of facial expression extracted automatically from video. <SPAN><SPAN class="ref-journal">Image and Vision Computing. </SPAN></SPAN> In Press.</SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R23"><SPAN class="ref-label">23.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446287">Wiskott L, et al.  Face recognition by elastic bunch graph matching. <SPAN><SPAN class="ref-journal">IEEE Trans on PAMI. </SPAN>1997</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R24"><SPAN class="ref-label">24.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446319">Zhang Z, et al.  <SPAN class="ref-journal">AFRG.</SPAN> 1998. Comparison Between Geometry-Based and Gabor-Wavelets-Based Facial Expression Recognition Using Multi-Layer Perceptron; pp. 454–459.</SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R25"><SPAN class="ref-label">25.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446357">Cohen I, et al.  Learning Bayesian Network Classifier for Facial Expression Recognition using both Labeled and Unlabeled Data. <SPAN><SPAN class="ref-journal">CVPR. </SPAN>2003</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R26"><SPAN class="ref-label">26.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446389">Wang Y, et al.  Real Time Facial Expression Recognition with AdaBoost. <SPAN><SPAN class="ref-journal">ICPR. </SPAN>2004</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R27"><SPAN class="ref-label">27.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446420">Cohen I, et al.  Facial expression recognition from video sequences: temporal and static modeling. <SPAN><SPAN class="ref-journal">Computer Vision and Image Understanding. </SPAN>2003;<SPAN class="ref-vol">91</SPAN>(1–2):160–187.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R28"><SPAN class="ref-label">28.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446463">Yacoob Y, Davis LS. Recognizing Human Facial Expressions From Long Image Sequeces Using Optical Flow. <SPAN><SPAN class="ref-journal">IEEE Trans on PAMI. </SPAN>1996;<SPAN class="ref-vol">18</SPAN>(6):636–642.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R29"><SPAN class="ref-label">29.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446511">Yeasin M, Bullot B, Sharma R. From facial expression to level of interest: a spatio-temporal approach. <SPAN><SPAN class="ref-journal">IEEE Conference on Computer Vision and Pattern Recognition. </SPAN>2004</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R30"><SPAN class="ref-label">30.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446556">Lien JJ, et al.  Detection, tracking, and classification of action units in facial expression. <SPAN><SPAN class="ref-journal">Robtics and Autonomous Systems. </SPAN>2000;<SPAN class="ref-vol">31</SPAN>:131–146.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R31"><SPAN class="ref-label">31.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446596">Chang Y, Hu C, Turk M. Probabilistic expression analysis on manifolds. <SPAN><SPAN class="ref-journal">CVPR. </SPAN>2004</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R32"><SPAN class="ref-label">32.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446640">Chang Y, et al.  Automatic 3D Facial Expression Analysis in Videos. <SPAN><SPAN class="ref-journal">IEEE International Workshop on Analysis and Modeling of Faces and Gestures. </SPAN>2005</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R33"><SPAN class="ref-label">33.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446673">Ohta H, Saji H, Nakatani H. Recognition of Facial Expressions Using Muscle-Based Feature Models. <SPAN><SPAN class="ref-journal">IEEE International Conference on Pattern Recognition. </SPAN>1998</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R34"><SPAN class="ref-label">34.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446717">Essa IA, Pentland AP. Coding, Analysis, Interpretation, and Recognition of Facial Expressions. <SPAN><SPAN class="ref-journal">IEEE Trans on PAMI. </SPAN>1997;<SPAN class="ref-vol">19</SPAN>(7):757–763.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R35"><SPAN class="ref-label">35.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446765">Essa IA, Pentland AP. Facial expression recognition using a dynamic model and motion energy. <SPAN><SPAN class="ref-journal">ICCV. </SPAN>1995:360–367.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R36"><SPAN class="ref-label">36.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446808">Kimura S, Yachida M. Facial expression recognition and its degree estimation. <SPAN><SPAN class="ref-journal">IEEE Conference on Computer Vision and Pattern Recognition. </SPAN>1997:295–300.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R37"><SPAN class="ref-label">37.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446851">Lanitis A, Taylor CJ, Cootes TF. Automatic Interpretation and Coding of Face Images Using Flexible Models. <SPAN><SPAN class="ref-journal">IEEE Trans on PAMI. </SPAN>1997;<SPAN class="ref-vol">19</SPAN>(7):743–756.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R38"><SPAN class="ref-label">38.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446906">Davatzikos C. Measuring biological shape using geometry-based shape transformations. <SPAN><SPAN class="ref-journal">Image and Vision Computing. </SPAN>2001;<SPAN class="ref-vol">19</SPAN>:63–74.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R39"><SPAN class="ref-label">39.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446945">Wen Z, Huang TS. Capturing subtle facial motions in 3D face tracking. <SPAN><SPAN class="ref-journal">IEEE International Conference on Computer Vision and Pattern Recognition. </SPAN>2003:1343–1350.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R40"><SPAN class="ref-label">40.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid446988">Cootes TF, Edwards GJ, Taylor CJ. Active Appearance Models. <SPAN><SPAN class="ref-journal">IEEE Trans on PAMI. </SPAN>2001;<SPAN class="ref-vol">23</SPAN>(6):681–685.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R41"><SPAN class="ref-label">41.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447042">Yang MH, Kriegman DJ, Ahuja N. Detecting Faces in Images: A Survey. <SPAN><SPAN class="ref-journal">IEEE Trans on PAMI. </SPAN>2002;<SPAN class="ref-vol">24</SPAN>(1):34–58.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R42"><SPAN class="ref-label">42.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447096">Viola P, Jones M. Robust Real-time Object Detection. <SPAN><SPAN class="ref-journal">International Journal of Computer Vision. </SPAN>2004;<SPAN class="ref-vol">57</SPAN>(2):137–154.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R43"><SPAN class="ref-label">43.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447144">Li SZ, Zhang Z. FloatBoost Learning and Statistical Face Detection. <SPAN><SPAN class="ref-journal">IEEE Trans on PAMI. </SPAN>2004;<SPAN class="ref-vol">26</SPAN>(9):1112–1123.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R44"><SPAN class="ref-label">44.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447192">Wang P, Ji Q. Learning Discriminant Features for Multi-View Face and Eye Detection. <SPAN><SPAN class="ref-journal">Computer Vision and Image Understanding. </SPAN>2007;<SPAN class="ref-vol">105</SPAN>(2):99–111.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R45"><SPAN class="ref-label">45.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447240">Stegmann BKE MB, Larsen R. FAME - a flexible appearance modelling environment. <SPAN><SPAN class="ref-journal">IEEE Trans on Medical Imaging. </SPAN>2003;<SPAN class="ref-vol">22</SPAN>(10):1319–1331.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R46"><SPAN class="ref-label">46.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447287">Corinna Cortes VV. Support-Vector Networks. <SPAN><SPAN class="ref-journal">Machine Learning. </SPAN>1995;<SPAN class="ref-vol">20</SPAN>(3):273–297.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R47"><SPAN class="ref-label">47.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447328">Kwok JTY. The evidence framework applied to support vector machines. <SPAN><SPAN class="ref-journal">IEEE Trans on Neural Networks. </SPAN>2000;<SPAN class="ref-vol">11</SPAN>(5):1162–1173.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R48"><SPAN class="ref-label">48.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447369">Platt J. <SPAN class="ref-journal">Probabilistic outputs for support vector machines and comparison to regularized likelihood methods.</SPAN> Cambridge, MA: MIT Press; 2000. </SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R49"><SPAN class="ref-label">49.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447403">Hsu C-JL C-W. A Comparison of Methods for Multiclass Support Vector Machines. <SPAN><SPAN class="ref-journal">IEEE Trans on Neural Networks. </SPAN>2002;<SPAN class="ref-vol">13</SPAN>(2)</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R50"><SPAN class="ref-label">50.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447438">Gur RC, Sara RMH, et al.  A method for obtaining 3-dimensional facial expressions and its standardization for the use in neurocognitive studies. <SPAN><SPAN class="ref-journal">Journal of Neuroscience Methods. </SPAN>2002;<SPAN class="ref-vol">115</SPAN>(2):137–143.</SPAN>  <SPAN style="white-space: nowrap;">[<A class="ref-extlink" href="http://www.ncbi.nlm.nih.gov/pubmed/11992665" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">PubMed</A>]</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R51"><SPAN class="ref-label">51.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447493">Kanade T, Cohn JF, Tian Y. Comprehensive Database for Facial Expression Analysis. <SPAN><SPAN class="ref-journal">AFRG. </SPAN>2000:46–53.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R52"><SPAN class="ref-label">52.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447542">Bartlett MS, et al.  Recognizing Facial Expression: Machine Learning and Application to Spontaneous Behavior. <SPAN><SPAN class="ref-journal">CVPR. </SPAN>2005:568–573.</SPAN></SPAN></SPAN></DIV><DIV class="ref-cit-blk" id="R53"><SPAN class="ref-label">53.</SPAN> <SPAN class="ref-cit"><SPAN class="citation" id="__citationid447579">Kohler CG, Turner, Travis, Stolar, Neal M, Bilker, Warren B, Brensinger, Colleen M, Gur, Raquel E, Gur, Ruben C. Differences in facial expressions of four universal emotions. <SPAN><SPAN class="ref-journal">Psychiatry Research. </SPAN>2004;<SPAN class="ref-vol">128</SPAN>(3):235–244.</SPAN>  <SPAN style="white-space: nowrap;">[<A class="ref-extlink" href="http://www.ncbi.nlm.nih.gov/pubmed/15541780" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article%7CCitationRef&amp;TO=Entrez%7CPubMed%7CRecord&amp;rendering-type=normal">PubMed</A>]</SPAN></SPAN></SPAN></DIV></DIV></DIV></DIV><SCRIPT>try{utils.jsLoader.sBase = "http://www.ncbi.nlm.nih.gov/corehtml/jsutils/";}catch(e){}</SCRIPT><SCRIPT type="text/javascript" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/tileshop_pmc.1.js"></SCRIPT><SCRIPT src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/firebugx.js" type="text/javascript"></SCRIPT><SCRIPT src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/tile.1.js" type="text/javascript"></SCRIPT><SCRIPT src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/notify.1.js" type="text/javascript"></SCRIPT><SCRIPT src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/drag_n_drop.1.js" type="text/javascript"></SCRIPT><SCRIPT src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/scale_pmc.1.js" type="text/javascript"></SCRIPT><SCRIPT type="text/javascript" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/tileshop_data_db.1.js"></SCRIPT><SCRIPT src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/data_provider.1.js" type="text/javascript"></SCRIPT><SCRIPT src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/remote_data_provider.1.js" type="text/javascript"></SCRIPT>
          <HR>
          <DIV class="footer-section sans90">
            

          </DIV>
        </TD>
        
        <TD valign="top" class="sidebar-cell">
          <DIV id="NCBIPageSection" class="port section x-panel" config="collapsed: false, closeable: false">
  <DIV class="x-panel-header x-unselectable">
    <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#" class="x-tool x-tool-toggle"></A>
    <H2 class="x-panel-header-text">PubMed articles by these authors</H2>
  </DIV>
  <DIV class="x-panel-body">
    <DIV class="brieflink window">
  <UL class="pop0">
    <LI class="item">
      <A href="http://www.ncbi.nlm.nih.gov/sites/entrez?cmd=search&db=PubMed&term=%20Wang%2BP[auth]" ref="reftype=authsrch&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|ByTheseAuthors&amp;TO=Entrez|Search|Author%20Search"> Wang, P.</A>
    </LI>
    <LI class="item">
      <A href="http://www.ncbi.nlm.nih.gov/sites/entrez?cmd=search&db=PubMed&term=%20Barrett%2BF[auth]" ref="reftype=authsrch&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|ByTheseAuthors&amp;TO=Entrez|Search|Author%20Search"> Barrett, F.</A>
    </LI>
    <LI class="item">
      <A href="http://www.ncbi.nlm.nih.gov/sites/entrez?cmd=search&db=PubMed&term=%20Martin%2BE[auth]" ref="reftype=authsrch&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|ByTheseAuthors&amp;TO=Entrez|Search|Author%20Search"> Martin, E.</A>
    </LI>
    <LI class="item">
      <A href="http://www.ncbi.nlm.nih.gov/sites/entrez?cmd=search&db=PubMed&term=%20Verma%2BR[auth]" ref="reftype=authsrch&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|ByTheseAuthors&amp;TO=Entrez|Search|Author%20Search"> Verma, R.</A>
    </LI>
  </UL>
</DIV>
  </DIV>
</DIV>

          <DIV id="NCBIPageSection" class="port norender brieflink" config="collapsed: false, closeable: false">
  <H2>PubMed related articles</H2>
  <DIV class="PPMCPubmedRA">
  <UL class="pop1">
    <LI class="item popnode note">
      <A href="http://www.ncbi.nlm.nih.gov/pubmed/17442398?ordinalpos=1&itool=PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCPubmedRA&linkpos=1" ref="reftype=relart&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Record">Computerized measurement of facial expression of emotions in schizophrenia.</A>
      <P class="desc">
        <EM class="cit">J Neurosci Methods. 2007 Jul 30; 163(2):350-61. Epub 2007 Mar 12.</EM>
      </P>
      <P class="note"> [J Neurosci Methods.  2007]</P>
    </LI>
    <LI class="item popnode note">
      <A href="http://www.ncbi.nlm.nih.gov/pubmed/17283776?ordinalpos=1&itool=PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCPubmedRA&linkpos=2" ref="reftype=relart&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Record">Facial expression recognition in image sequences using geometric deformation features and Support Vector Machines.</A>
      <P class="desc">
        <EM class="cit">IEEE Trans Image Process. 2007 Jan; 16(1):172-87. </EM>
      </P>
      <P class="note"> [IEEE Trans Image Process.  2007]</P>
    </LI>
    <LI class="item popnode note">
      <A href="http://www.ncbi.nlm.nih.gov/pubmed/19163791?ordinalpos=1&itool=PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCPubmedRA&linkpos=3" ref="reftype=relart&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Record">Objective grading of facial paralysis using Local Binary Patterns in video processing.</A>
      <P class="desc">
        <EM class="cit">Conf Proc IEEE Eng Med Biol Soc. 2008; 2008:4805-8. </EM>
      </P>
      <P class="note"> [Conf Proc IEEE Eng Med Biol Soc.  2008]</P>
    </LI>
    <LI class="item popnode note">
      <A href="http://www.ncbi.nlm.nih.gov/pubmed/15201491?ordinalpos=1&itool=PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCPubmedRA&linkpos=4" ref="reftype=relart&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Record"><SPAN class="reviewflag">Review</SPAN>Laterality of facial expressions of emotion: Universal and culture-specific influences.</A>
      <P class="desc">
        <EM class="cit">Behav Neurol. 2004; 15(1-2):23-34. </EM>
      </P>
      <P class="note"> [Behav Neurol.  2004]</P>
    </LI>
    <LI class="item popnode note">
      <A href="http://www.ncbi.nlm.nih.gov/pubmed/8165273?ordinalpos=1&itool=PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCPubmedRA&linkpos=5" ref="reftype=relart&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Record"><SPAN class="reviewflag">Review</SPAN>Innate and universal facial expressions: evidence from developmental and cross-cultural research.</A>
      <P class="desc">
        <EM class="cit">Psychol Bull. 1994 Mar; 115(2):288-99. </EM>
      </P>
      <P class="note"> [Psychol Bull.  1994]</P>
    </LI>
    <LI class="more">» <A href="http://www.ncbi.nlm.nih.gov/sites/entrez?db=pubmed&cmd=link&linkname=pubmed_pubmed_reviews&uid=18045693&ordinalpos=1&log$=relatedarticles&logdbfrom=pmc" ref="reftype=relart&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Reviews">See reviews...</A>  |  » <A href="http://www.ncbi.nlm.nih.gov/sites/entrez?db=pubmed&cmd=link&linkname=pubmed_pubmed&uid=18045693&ordinalpos=1&log$=relatedarticles&logdbfrom=pmc" ref="reftype=relart&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|RelatedArticles&amp;TO=Entrez|Pubmed|Related%20Records">See all...</A></LI>
  </UL>
</DIV>
</DIV>

          <DIV id="recent-activity">
            <DIV id="NCBIPageSection" class="port section x-panel" config="collapsed: false, closeable: false">
  <DIV class="x-panel-header x-unselectable">
    <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#" class="x-tool x-tool-toggle"></A>
    <H2 class="x-panel-header-text">Recent Activity</H2>
  </DIV>
  <DIV class="x-panel-body">
    <DIV class="window">
      <DIV id="HTDisplay" class="">
        <INPUT name="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.Cmd" sid="1" type="hidden">
        <DIV class="action">
          <A name="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.ClearHistory" sid="1" realname="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.ClearHistory" cmd="ClearHT" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/?cmd=ClearHT&" onclick="return false;" id="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.ClearHistory">
                        Clear
                    </A>
          <A name="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.HistoryToggle" sid="1" realname="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.HistoryToggle" class="HTOn" cmd="HTOff" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/?cmd=HTOff&" onclick="return false;" id="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.HistoryToggle">
                        Turn Off                        
                    </A>
          <A name="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.HistoryToggle" sid="2" realname="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.HistoryToggle" class="HTOff" cmd="HTOn" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/?cmd=HTOn&" onclick="return false;" id="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.HistoryToggle">
                        Turn On                        
                    </A>
        </DIV>
        <UL id="activity">
          <LI class="record popnode rapopper"> <DIV><A class="htb" href="http://www.ncbi.nlm.nih.gov/portal/utils/pageresolver.fcgi?log$=activity&recordid=1262878029346956" title="J Neurosci Methods. 2008 Feb 15; 168(1): 224-238.">Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders</A><A class="hidden">Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders</A></DIV></LI>
        </UL>
        <P class="HTOn">Your browsing activity is empty.</P>
        <P class="HTOff">Activity recording is turned off.</P>
        <P id="turnOn" class="HTOff">
          <A name="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.HistoryOn" sid="1" realname="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.HistoryOn" cmd="HTOn" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/?cmd=HTOn&" onclick="return false;" id="PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCRecentActivity.HistoryOn">Turn recording back on</A>
        </P>
      </DIV>
    </DIV>
  </DIV>
</DIV>

          </DIV>
          <DIV id="NCBIPageSection" class="port section x-panel" config="collapsed: false, closeable: false">
  <DIV class="x-panel-header x-unselectable">
    <A href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2238802/#" class="x-tool x-tool-toggle"></A>
    <H2 class="x-panel-header-text">Links</H2>
  </DIV>
  <DIV class="x-panel-body">
    <DIV class="brieflink window">
  <UL class="pop0">
    <LI class="item">
      <A href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&DbFrom=pmc&Cmd=Link&LinkName=pmc_pubmed&LinkReadableName=PubMed&IdsFromResult=2238802&ordinalpos=1&itool=PPMCLayout.PPMCAppController.PPMCArticlePage.PPMCDiscoveryDbLinks" ref="reftype=PubMed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|DiscoveryLinks&amp;TO=Entrez|PubMed|Record">PubMed</A>
    </LI>
  </UL>
</DIV>
  </DIV>
</DIV>

        </TD>
      </TR>
    </TBODY></TABLE>
    
    
<DIV class="ArticleRefDocsums" id="ArticleRefDocsums"><DIV id="cite-P3" rid="P3" class="cited-ref" pmids="3291095 9718632 16452608 15585289 17563202" style="top: 976px; left: 586px; "><UL><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/3291095" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record"><SPAN class="flag">Review</SPAN>Deficits in facial-affect recognition and schizophrenia.</A><P class="desc"><EM class="cit">Schizophr Bull. 1988; 14(1):67-83.</EM></P><SPAN class="pub"> [Schizophr Bull.  1988]</SPAN></LI><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/9718632" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record"><SPAN class="flag">Review</SPAN>Facial expressions of emotions and schizophrenia: a review.</A><P class="desc"><EM class="cit">Schizophr Bull. 1998; 24(3):399-412.</EM></P><SPAN class="pub"> [Schizophr Bull.  1998]</SPAN></LI><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/16452608" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">Flat affect in schizophrenia: relation to emotion processing and neurocognitive measures.</A><P class="desc"><EM class="cit">Schizophr Bull. 2006 Apr; 32(2):279-87.</EM></P><SPAN class="pub"> [Schizophr Bull.  2006]</SPAN></LI><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/15585289" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">Quantification of facial expressions using high-dimensional shape transformations.</A><P class="desc"><EM class="cit">J Neurosci Methods. 2005 Jan 30; 141(1):61-73.</EM></P><SPAN class="pub"> [J Neurosci Methods.  2005]</SPAN></LI><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/17563202" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">The Facial Expression Coding System (FACES): development, validation, and utility.</A><P class="desc"><EM class="cit">Psychol Assess. 2007 Jun; 19(2):210-24.</EM></P><SPAN class="pub"> [Psychol Assess.  2007]</SPAN></LI></UL><A class="all-articles" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record" href="http://www.ncbi.nlm.nih.gov/pubmed/3291095,9718632,16452608,15585289,17563202/?report=summary" style="display: none; ">See more articles cited in this paragraph</A></DIV><DIV id="cite-P4" rid="P4" class="cited-ref" pmids="1390955 15585289" style="top: 1435px; left: 586px; "><UL><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/1390955" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">Facial expression and emotional face recognition in schizophrenia and depression.</A><P class="desc"><EM class="cit">Eur Arch Psychiatry Clin Neurosci. 1992; 242(1):46-52.</EM></P><SPAN class="pub"> [Eur Arch Psychiatry Clin Neurosci.  1992]</SPAN></LI><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/15585289" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">Quantification of facial expressions using high-dimensional shape transformations.</A><P class="desc"><EM class="cit">J Neurosci Methods. 2005 Jan 30; 141(1):61-73.</EM></P><SPAN class="pub"> [J Neurosci Methods.  2005]</SPAN></LI></UL><A class="all-articles" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record" href="http://www.ncbi.nlm.nih.gov/pubmed/1390955,15585289/?report=summary" style="display: none; ">See more articles cited in this paragraph</A></DIV><DIV id="cite-P7" rid="P7" class="cited-ref" pmids="17563202" style="top: 2236px; left: 586px; "><UL><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/17563202" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">The Facial Expression Coding System (FACES): development, validation, and utility.</A><P class="desc"><EM class="cit">Psychol Assess. 2007 Jun; 19(2):210-24.</EM></P><SPAN class="pub"> [Psychol Assess.  2007]</SPAN></LI></UL><A class="all-articles" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record" href="http://www.ncbi.nlm.nih.gov/pubmed/17563202/?report=summary" style="display: none; ">See more articles cited in this paragraph</A></DIV><DIV id="cite-P9" rid="P9" class="cited-ref" pmids="10194972" style="top: 2976px; left: 586px; "><UL><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/10194972" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">Measuring facial expressions by computer image analysis.</A><P class="desc"><EM class="cit">Psychophysiology. 1999 Mar; 36(2):253-63.</EM></P><SPAN class="pub"> [Psychophysiology.  1999]</SPAN></LI></UL><A class="all-articles" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record" href="http://www.ncbi.nlm.nih.gov/pubmed/10194972/?report=summary" style="display: none; ">See more articles cited in this paragraph</A></DIV><DIV id="cite-P11" rid="P11" class="cited-ref" pmids="15585289" style="top: 4004px; left: 586px; "><UL><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/15585289" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">Quantification of facial expressions using high-dimensional shape transformations.</A><P class="desc"><EM class="cit">J Neurosci Methods. 2005 Jan 30; 141(1):61-73.</EM></P><SPAN class="pub"> [J Neurosci Methods.  2005]</SPAN></LI></UL><A class="all-articles" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record" href="http://www.ncbi.nlm.nih.gov/pubmed/15585289/?report=summary" style="display: none; ">See more articles cited in this paragraph</A></DIV><DIV id="cite-P30" rid="P30" class="cited-ref" pmids="11992665" style="top: 10899px; left: 586px; "><UL><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/11992665" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">A method for obtaining 3-dimensional facial expressions and its standardization for use in neurocognitive studies.</A><P class="desc"><EM class="cit">J Neurosci Methods. 2002 Apr 15; 115(2):137-43.</EM></P><SPAN class="pub"> [J Neurosci Methods.  2002]</SPAN></LI></UL><A class="all-articles" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record" href="http://www.ncbi.nlm.nih.gov/pubmed/11992665/?report=summary" style="display: none; ">See more articles cited in this paragraph</A></DIV><DIV id="cite-P31" rid="P31" class="cited-ref" pmids="11992665" style="top: 11158px; left: 586px; "><UL><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/11992665" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">A method for obtaining 3-dimensional facial expressions and its standardization for use in neurocognitive studies.</A><P class="desc"><EM class="cit">J Neurosci Methods. 2002 Apr 15; 115(2):137-43.</EM></P><SPAN class="pub"> [J Neurosci Methods.  2002]</SPAN></LI></UL><A class="all-articles" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record" href="http://www.ncbi.nlm.nih.gov/pubmed/11992665/?report=summary" style="display: none; ">See more articles cited in this paragraph</A></DIV><DIV id="cite-P32" rid="P32" class="cited-ref" pmids="11992665" style="top: 11582px; left: 586px; "><UL><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/11992665" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">A method for obtaining 3-dimensional facial expressions and its standardization for use in neurocognitive studies.</A><P class="desc"><EM class="cit">J Neurosci Methods. 2002 Apr 15; 115(2):137-43.</EM></P><SPAN class="pub"> [J Neurosci Methods.  2002]</SPAN></LI></UL><A class="all-articles" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record" href="http://www.ncbi.nlm.nih.gov/pubmed/11992665/?report=summary" style="display: none; ">See more articles cited in this paragraph</A></DIV><DIV id="cite-P33" rid="P33" class="cited-ref" pmids="11992665" style="top: 12182px; left: 586px; "><UL><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/11992665" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">A method for obtaining 3-dimensional facial expressions and its standardization for use in neurocognitive studies.</A><P class="desc"><EM class="cit">J Neurosci Methods. 2002 Apr 15; 115(2):137-43.</EM></P><SPAN class="pub"> [J Neurosci Methods.  2002]</SPAN></LI></UL><A class="all-articles" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record" href="http://www.ncbi.nlm.nih.gov/pubmed/11992665/?report=summary" style="display: none; ">See more articles cited in this paragraph</A></DIV><DIV id="cite-P34" rid="P34" class="cited-ref" pmids="15585289" style="top: 12463px; left: 586px; "><UL><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/15585289" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">Quantification of facial expressions using high-dimensional shape transformations.</A><P class="desc"><EM class="cit">J Neurosci Methods. 2005 Jan 30; 141(1):61-73.</EM></P><SPAN class="pub"> [J Neurosci Methods.  2005]</SPAN></LI></UL><A class="all-articles" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record" href="http://www.ncbi.nlm.nih.gov/pubmed/15585289/?report=summary" style="display: none; ">See more articles cited in this paragraph</A></DIV><DIV id="cite-P35" rid="P35" class="cited-ref" pmids="17563202" style="top: 13292px; left: 586px; "><UL><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/17563202" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">The Facial Expression Coding System (FACES): development, validation, and utility.</A><P class="desc"><EM class="cit">Psychol Assess. 2007 Jun; 19(2):210-24.</EM></P><SPAN class="pub"> [Psychol Assess.  2007]</SPAN></LI></UL><A class="all-articles" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record" href="http://www.ncbi.nlm.nih.gov/pubmed/17563202/?report=summary" style="display: none; ">See more articles cited in this paragraph</A></DIV><DIV id="cite-P47" rid="P47" class="cited-ref" pmids="15541780" style="top: 17988px; left: 586px; "><UL><LI class="ovfl"><A class="pl popnode" href="http://www.ncbi.nlm.nih.gov/pubmed/15541780" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record">Differences in facial expressions of four universal emotions.</A><P class="desc"><EM class="cit">Psychiatry Res. 2004 Oct 30; 128(3):235-44.</EM></P><SPAN class="pub"> [Psychiatry Res.  2004]</SPAN></LI></UL><A class="all-articles" ref="reftype=pubmed&amp;article-id=2238802&amp;issue-id=161092&amp;journal-id=319&amp;FROM=Article|CitedRefBlock&amp;TO=Entrez|Pubmed|Record" href="http://www.ncbi.nlm.nih.gov/pubmed/15541780/?report=summary" style="display: none; ">See more articles cited in this paragraph</A></DIV></DIV>


    <DIV xmlns="http://www.w3.org/1999/xhtml" class="footer" id="footer" xml:base="http://127.0.0.1/sites/static/header_footer?Ncbi_App=pmc&amp;Db=pmc&amp;Page=article&amp;Time=2010-01-07T10:27:09-05:00&amp;Host=ptpmc2&amp;Snapshot=PPMC@131073">
    <DIV class="crest"></DIV>
    <DIV class="breadcrumbs">You are here: <SPAN id="breadcrumb_text"><A>NCBI</A>
                    &gt; <A href="http://www.ncbi.nlm.nih.gov/guide/literature/">Literature</A>
                    
                    &gt; PubMed Central</SPAN></DIV>
    <A href="http://www.ncbi.nlm.nih.gov/sites/ehelp?Ncbi_App=pmc&Db=pmc&Page=article&Snapshot=PPMC@131073&Time=2010-01-07T10:27:09-05:00&Host=ptpmc2" onclick="window.open(&#39;/sites/ehelp?Ncbi_App=pmc&amp;Db=pmc&amp;Page=article&amp;Snapshot=PPMC@131073&amp;Time=2010-01-07T10:27:09-05:00&amp;Host=ptpmc2&#39;,&#39;HelpDesk&#39;,&#39;resizable=no,scrollbars=yes,location=no,status=yes,menubar=no,top=50,left=200,width=720,height=695&#39;);return false;" id="help-desk-link" class="help_desk" target="_blank">Help Desk</A>
    <DIV class="subfooter">
        <H2 class="offscreen_noflow">Simple NCBI Directory</H2>
        <UL class="foot_list" id="getting_started">
            <LI><H3>Getting Started</H3></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/guide/all/">Site Map</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/bookshelf/br.fcgi?book=helpcollect">NCBI Help Manual</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/books/bv.fcgi?rid=handbook.TOC&depth=2">NCBI Handbook</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/guide/training-tutorials/">Training &amp; Tutorials</A></LI>
        </UL>
        
        <UL class="foot_list" id="resources"><LI><H3>
                        Resources
</H3></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/literature/">Literature</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/dna-rna/">DNA &amp; RNA</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/proteins/">Proteins</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/sequence-analysis/">Sequence Analysis</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/genes-expression/">Genes &amp; Expression</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/genomes/">Genomes</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/maps-markers/">Maps &amp; Markers</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/domains-structures/">Domains &amp; Structures</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/genetics-medicine/">Genetics &amp; Medicine</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/taxonomy/">Taxonomy</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/data-software/">Data &amp; Software</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/training-tutorials/">Training &amp; Tutorials</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/homology/">Homology</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/small-molecules/">Small Molecules</A></LI><LI><A href="http://www.ncbi.nlm.nih.gov/guide/variation/">Variation</A></LI></UL>
        <UL class="foot_list" id="popular">
            <LI><H3>Popular</H3></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/pubmed/">PubMed</A></LI>
            <LI><A href="http://www.pubmedcentral.nih.gov/" class="newdomain">PubMed Central</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/books/">Bookshelf</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/BLAST/">BLAST</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/gene/">Gene</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/nucleotide/">Nucleotide</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/protein/">Protein</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/geo/">GEO</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml">Conserved Domains</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/Structure/">Structure</A></LI>
            <LI><A href="http://pubchem.ncbi.nlm.nih.gov/" class="newdomain">PubChem</A></LI>
        </UL>
        <UL class="foot_list" id="featured">
            <LI><H3>Featured</H3></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/Genbank/">GenBank</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/refseq/">Reference Sequences</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/mapview/">Map Viewer</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/genomeprj">Genome Projects</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/genome/guide/human/">Human Genome</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/genome/guide/mouse/">Mouse Genome</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/genomes/FLU/">Influenza Virus</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/tools/primer-blast/">Primer-BLAST</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/Traces/sra/">Short Read Archive</A></LI>
        </UL>
        <UL class="foot_list" id="info">
            <LI><H3>NCBI Information</H3></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/About/">About NCBI</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/CBBresearch/">Research at NCBI</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/bookshelf/br.fcgi?book=newsncbi">NCBI Newsletter</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/Ftp/">NCBI FTP Site</A></LI>
            <LI><A href="http://www.ncbi.nlm.nih.gov/About/glance/contact_info.html">Contact Us</A></LI>
        </UL>
    </DIV>
    
    
    <DIV id="ncbifooter" class="contact_info">
        <DIV id="footer-contents-right">
            <A href="http://www.nih.gov/" title="NIH" class="nih_img_link newdomain">NIH</A>
            <A href="http://www.dhhs.gov/" title="DHHS" class="dhhs_img_link newdomain">DHHS</A>
            <A href="http://www.usa.gov/" title="USA.gov" class="usagov_img_link newdomain">USA.gov</A>
            
        </DIV>
        <DIV id="footer-contents-left">
            
            <A href="http://www.ncbi.nlm.nih.gov/About/disclaimer.html">Copyright</A> | <A href="http://www.ncbi.nlm.nih.gov/About/disclaimer.html#disclaimer">Disclaimer</A> |
            <A href="http://www.nlm.nih.gov/privacy.html" class="newdomain">Privacy</A> | <A href="http://www.ncbi.nlm.nih.gov/About/accessibility.html">Accessibility</A> | <A href="http://www.ncbi.nlm.nih.gov/About/glance/contact_info.html">Contact</A>
            <P class="address vcard">
                <SPAN class="url">
                    <A class="fn url newdomain" href="http://www.ncbi.nlm.nih.gov/">National Center for
                        Biotechnology Information</A>
                </SPAN>, <SPAN class="org">U.S. National Library of Medicine</SPAN>
                <SPAN class="adr">
                    <SPAN class="street-address">8600 Rockville Pike</SPAN>, <SPAN class="locality">Bethesda</SPAN>
                    <SPAN class="region">MD</SPAN>, <SPAN class="postal-code">20894</SPAN>
                    <SPAN class="country-name">USA</SPAN>
                </SPAN>
            </P>
        </DIV>
    </DIV>
    <SCRIPT type="text/javascript" src="./Automated Video Based Facial Expression Analysis of Neuropsychiatric Disorders_files/InstrumentPageStarterJS.js"></SCRIPT>
</DIV>
    
    <SPAN class="PAFAppResources"></SPAN>
    
    
    
    <!--
      {
        HttpRequest-AppName: 'ppmc',
        BaseURL:  'http://www.ncbi.nlm.nih.gov',
        SelectedBackend: 'bePMCRenderer'
      }
    -->

  <!--84DA06D4B45FD4C1_0182SID:PPMC:3.1:ptpmc2:v2.7.r173190: Fri, Oct 16 2009 11:58:10-->

<DIV id="popper0" class="popper popnode"><DIV class="popperInnerDiv"></DIV></DIV><DIV id="popper1" class="popper popnode"><DIV class="popperInnerDiv"></DIV></DIV><DIV id="popper2" class="popper popnode"><DIV class="popperInnerDiv"></DIV></DIV><DIV id="popper3" class="popper popnode"><DIV class="popperInnerDiv"></DIV></DIV><DIV id="popper4" class="popper popnode"><DIV class="popperInnerDiv"></DIV></DIV><DIV id="popper-port-links" class="popper popnode"><DIV class="popperInnerDiv"></DIV></DIV><DIV id="popper-para-links" class="popper popnode" style="left: 53px; top: 5063px; "><DIV class="popperInnerDiv">15. Christopher Alvino CK, Barrett Frederick, Gur Raquel E, Gur Ruben C, Verma Ragini. Journal of Neuroscience Methods. 2007. Computerized Measurement of Facial Expression of Emotions in Schizophrenia. [Reference List]</DIV></DIV></BODY></HTML>